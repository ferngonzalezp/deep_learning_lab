{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DL_labSession2.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOXQ2ahZ3PdYqL6A70Jsdcx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ferngonzalezp/deep_learning_lab/blob/main/session2/DL_labSession2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QnWRrOhkHfwi"
      },
      "source": [
        "# Introduction to Convolutional Neural Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m5lK7-NbHorr"
      },
      "source": [
        "Convolutional neural networks or more commonly called by their abreviation \"CNNs\", is one of the most used architecure types of Neural Networks. These networks were inspired by the field of image processing, they use as building block the convolution operation instead of simple matrix multiplications. The use of convolutions have some advantages, such as:\n",
        "\n",
        "* Reduced memory requirements: with convolutions the weights are filter or kernels that pass through the whole image, for example a 3x3 kernel has in total 9 weights and can be used in any image whose dimensions are equal or bigger than the ones from the kernel, if we used a linear layer the number of weights increase linearly with the number of pixels in the image.\n",
        "\n",
        "* Convolutions are equivariant: this means that with convolutions we are able to identify features of an image regardless of their orientation. With a trained CNN for recognizing MNIST digits we can rotate the numbers or crop them and still be able to identify the number, this would be a hard task for linear layers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lDkXNt0BVSTS"
      },
      "source": [
        "## What is a Convolution extacly?\n",
        "\n",
        "A convolution is the process of adding each element of the image to its local neighbors, weighted by the kernel. \n",
        "\n",
        "<img src='https://miro.medium.com/max/2400/1*VVvdh-BUKFh2pwDD0kPeRA@2x.gif'\n",
        "width=500 height=300>\n",
        "\n",
        "Convolutions also have certain parameters that control how they behave and the shape of their output:\n",
        "\n",
        "* **Stride**:  specifies how much we move the convolution filter at each step.\n",
        "\n",
        "Convolution with stride of 1: \\\\\n",
        "<img src='https://miro.medium.com/max/2400/1*L4T6IXRalWoseBncjRr4wQ@2x.gif'\n",
        "width=500 height=300> \\\\\n",
        "Convolution with stride of 2: \\\\\n",
        "<img src='https://miro.medium.com/max/2400/1*4wZt9G7W7CchZO-5rVxl5g@2x.gif'\n",
        "width=500 height=300> \\\\\n",
        "\n",
        "* **Padding**: these are extra values added arround the input image in order to achieve a desirable size of the reuslting operation. Normally the padding values are zeroes but others can be used like the values at the opposite edge.\\\\\n",
        "\n",
        "<img src='https://miro.medium.com/max/2400/1*W2D564Gkad9lj3_6t9I2PA@2x.gif'\n",
        "width=500 height=300> \\\\\n",
        "\n",
        "* **Dilation**: The dilations is the ammount of gap between the kernle weights, it can be seen as expanding the kernel to a bigger one and filling the gaps with zeroes. The default value of dilation is 1. \\\\\n",
        "\n",
        "<img src='https://www.researchgate.net/publication/320195101/figure/fig2/AS:669211164692494@1536563783748/Dilated-convolution-On-the-left-we-have-the-dilated-convolution-with-dilation-rate-r.png' width=600 height=300>\\\\\n",
        "\n",
        "The dimension of the output of a convolution can be calculated as:\n",
        "\n",
        "\\begin{equation}\n",
        "D_{o u t}=\\left\\lfloor\\frac{D_{i n}+2 \\times \\text { padding }-\\text { dilation } \\times(\\text { kernel_size }-1)-1}{\\text { stride }}+1 \\right\\rfloor\n",
        "\\end{equation}\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KOf8XyYZVZ8J"
      },
      "source": [
        "## Convolutional Neural Network for MNIST digits classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_yg97lraXsR-"
      },
      "source": [
        "Now we will try to solve the MNIST digit classification problem but using convolutions, we hope that by doing this we can increase accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "035nGIiOHZi7"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lIfob-2fW5-e"
      },
      "source": [
        "If the dataset download doesn't work you can download the dataset from [here](http://yann.lecun.com/exdb/mnist/). Then save the .gz fikles under ./MNIST/raw/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FDjhPx1U5Lqu"
      },
      "source": [
        "!git clone https://github.com/ferngonzalezp/MNIST"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A10X-J5aYA7C"
      },
      "source": [
        "transform = torchvision.transforms.ToTensor\n",
        "train_dataset = datasets.MNIST(root='./',download=True, train=True, transform=transform())\n",
        "test_dataset = datasets.MNIST(root='./', train=False, transform=transform())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AR8eLOAQZHXb"
      },
      "source": [
        "example = train_dataset[np.random.randint(0,len(train_dataset))]\n",
        "plt.imshow(example[0][0], cmap='gray')\n",
        "plt.title(example[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rtG862P9e9Vc"
      },
      "source": [
        "We will build a simple convolutional neural network for this task. The idea behind this CNN is to have some feature extraction layers using convolutions and then a fully connected output layer to compute the prediction of the labels.\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1x989PyIOJSE3gCUaM2DmmOMKYLnsvx9w' width=800 height=400> \\\\\n",
        "\n",
        "To reduce the dimensionality of the image we use a pooling operation.  This enables us to reduce the number of parameters, which both shortens the training time and combats overfitting. Pooling layers downsample each feature map independently, reducing the height and width, keeping the depth intact.\n",
        "\n",
        "The most common type of pooling is max pooling which just takes the max value in the pooling window. Contrary to the convolution operation, pooling has no parameters. It slides a window over its input, and simply takes the max value in the window. Similar to a convolution, we specify the window size and stride.\n",
        "\n",
        "Here is the result of max pooling using a 2x2 window and stride 2. Each color denotes a different window. Since both the window size and stride are 2, the windows are not overlapping. \\\\\n",
        "\n",
        "<img src='https://miro.medium.com/max/2400/1*ReZNSf_Yr7Q1nqegGirsMQ@2x.png' widht=600 height=300> \\\\\n",
        "\n",
        "The activation we use between convolutions is the rectified Linear unit (ReLU) and softmax in the output. We build our CNN pytorch module as follows: \\\\"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a5G1Mfd1YR76"
      },
      "source": [
        "class cnn(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(cnn,self).__init__()\n",
        "    self.feature_extraction = nn.Sequential(\n",
        "        nn.Conv2d(1,16,kernel_size=3,stride=1,padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "        nn.Conv2d(16,32,kernel_size=3,stride=1,padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "    )\n",
        "    self.fc = nn.Sequential(\n",
        "        nn.Linear(32*7*7,10),\n",
        "        nn.Softmax(dim=1)\n",
        "    )\n",
        "  def forward(self,x):\n",
        "    x = self.feature_extraction(x)\n",
        "    x = torch.flatten(x,1)\n",
        "    return self.fc(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LNDzLTPLjiIs"
      },
      "source": [
        "Now the remaining part of the code remains the same just as in classification with MLP.\n",
        "____"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZQ1OggBzcv8U"
      },
      "source": [
        "#Use GPU if available\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yICEVx96bnl0"
      },
      "source": [
        "model = cnn().to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uKH7O4HAbq0c"
      },
      "source": [
        "print(model)\n",
        "pytorch_total_params = sum(p.numel() for p in model.parameters())\n",
        "print(\"number of parameters in model: %d\"%(pytorch_total_params))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8R36CSmvc26J"
      },
      "source": [
        "#Hyperparameters\n",
        "batch_size = 100\n",
        "n_epochs = 10\n",
        "learn_rate = 2e-1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TBTiEc13c6M9"
      },
      "source": [
        "#DataLoaders\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8CQgmx71c86k"
      },
      "source": [
        "#Loss function and Optimizer\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(),lr=learn_rate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OrZ6XcfKc_Fk"
      },
      "source": [
        "#Training Loop\n",
        "train_loss = []\n",
        "val_loss = []\n",
        "for epoch in range(n_epochs):  # loop over the dataset multiple times\n",
        "    epoch_loss = 0.0\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(train_loader, 0):\n",
        "        # get the inputs; data is a list of [inputs, labels]\n",
        "        inputs, labels = data\n",
        "        inputs = inputs.to(device)\n",
        "        labels = F.one_hot(labels,10).type_as(inputs)\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "        # print statistics\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        if i % 50 == 49:    # print every 50 mini-batches\n",
        "            print('[%d, %5d] loss: %.3f' %\n",
        "                  (epoch + 1, i + 1, running_loss / 50))\n",
        "            running_loss = 0.0\n",
        "    train_loss.append(epoch_loss/(i+1))\n",
        "    #Evaluation of the trained model\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    epoch_loss = 0.0\n",
        "    print(\"validating...\")\n",
        "    with torch.no_grad():\n",
        "        for i, data in enumerate(test_loader, 0):\n",
        "            inputs, labels = data\n",
        "            inputs = inputs.to(device)\n",
        "            outputs = model(inputs)\n",
        "            predicted = torch.argmax(outputs,dim=1)\n",
        "            loss = criterion(outputs, F.one_hot(labels,10).type_as(inputs))\n",
        "            labels =  labels.type_as(inputs)\n",
        "            total += labels.shape[0]\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            epoch_loss += loss.item()\n",
        "    print('Accuracy of the network on the test images: %d %%' % (\n",
        "        100 * correct / total))\n",
        "    val_loss.append(epoch_loss/(i+1))\n",
        "\n",
        "print('Finished Training')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ICBze3WJdUmt"
      },
      "source": [
        "plt.plot(train_loss, label='training')\n",
        "plt.plot(val_loss, label='validation')\n",
        "plt.legend()\n",
        "plt.title('Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ff9IY2oIjyMC"
      },
      "source": [
        "We can observe thet we achieve a higher accuracy with a CNN and with less training, even after the first epoch we got the same accuracy of the treined MLP."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TDpuBjU0dXHc",
        "outputId": "7dd92130-7873-4e64-991b-d5f7e06e48dc"
      },
      "source": [
        "#Evaluation of the trained model\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for i, data in enumerate(test_loader, 0):\n",
        "        inputs, labels = data\n",
        "        inputs = inputs.to(device)\n",
        "        labels =  labels.type_as(inputs)\n",
        "        outputs = model(inputs)\n",
        "        predicted = torch.argmax(outputs,dim=1)\n",
        "        total += labels.shape[0]\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print('Accuracy of the network on the test images: %d %%' % (\n",
        "    100 * correct / total))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of the network on the test images: 98 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I1cBu14ekBLU"
      },
      "source": [
        "Let's plot some image to see how the model performs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "veuoQ101dZf8"
      },
      "source": [
        "from math import ceil\n",
        "n_samples = 6\n",
        "id = np.random.randint(0,len(test_dataset),n_samples)\n",
        "rows = ceil(n_samples/3)\n",
        "plt.figure(figsize=(rows*5,10))\n",
        "for i in range(len(id)):\n",
        "  images = test_dataset[id[i]][0].to(device)\n",
        "  pred_labels = torch.argmax(model(images.unsqueeze(0)),dim=1)[0]\n",
        "  plt.subplot(rows,3,i+1)\n",
        "  plt.imshow(images[0].cpu(), cmap='gray')\n",
        "  plt.title('prediction: %d'%(pred_labels))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XnuDHl6_Yfv8"
      },
      "source": [
        "# Tricks that help with performance, convergence and generalization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NgDDv3qaYpnz"
      },
      "source": [
        "Using convolutional layers helps a lot in the performance of commputer vision, however for real wolrs applications and more difficult tasks it may not be enough. For that reason there exists a bag of tricks that drive CNNs towards greater accuracy, generalization and capacity for handling more complicated tasks. Here we will talk about some of the most used that are present nowadays in most of deep learning models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CaDKqn3haGdo"
      },
      "source": [
        "## Batch Normalization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_6it1-DIgbSX"
      },
      "source": [
        "Batch normalization is used to fix the variance of the input of an activation function. This helps in reducuing the internal covariance shift in a neural networks as well as more practical benefits such as reducing the probability of having vanishing or exploding gradients and reducing generalization error. What BatchNorm does is first it calculates the mean and variance of the current mini-batch:\n",
        "\n",
        "\\begin{equation}\n",
        "\\mu_{B}=\\frac{1}{m} \\sum_{i=1}^{m} x_{i} \\text { , and } \\sigma_{B}^{2}=\\frac{1}{m} \\sum_{i=1}^{m}\\left(x_{i}-\\mu_{B}\\right)^{2}\n",
        "\\end{equation}\n",
        "\n",
        "Then normalizes the input of the layer as:\n",
        "\n",
        "\\begin{equation}\n",
        "\\hat{x}_{i}^{(k)}=\\frac{x_{i}^{(k)}-\\mu_{B}^{(k)}}{\\sqrt{\\sigma_{B}^{(k)^{2}}+\\epsilon}}\n",
        "\\end{equation}\n",
        "\n",
        "Where $k \\in[1, d] \\text { is the dimension number and } i \\in[1, m] \\text{ is the ith layer of the network}; \\mu_{B}^{(k)} \\text { and } \\sigma_{B}^{(k)} \\text{ are the per dimension mean and variance}$\n",
        "\n",
        "$\\epsilon$ is added in the denominator for numerical stability and is an arbitrarily small constant. The resulting normalized activation $\\hat{x}_{i}^{(k)}$ have zero mean and unit variance if $\\epsilon$ is not taken into account. To restore the representation power of the network, a transformation step then follows as:\n",
        "\n",
        "\\begin{equation}\n",
        "y_{i}^{(k)}=\\gamma^{(k)} \\hat{x}_{i}^{(k)}+\\beta^{(k)}\n",
        "\\end{equation}\n",
        "\n",
        "Where $\\gamma$ and $\\beta$ are learnable parameters. Implementing BatchNorm is very straightforward in pytorch. In [pytorch](https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html#torch.nn.BatchNorm2d) we have the module **BatchNorm1d, BatchNorm2d, BatchNorm3d** depending in the dimensionality of our input, we need also to specify the number of features.  In a module we can use batchNorm like this:\n",
        "\n",
        "\n",
        "```\n",
        "self.feature_extraction = nn.Sequential(\n",
        "        nn.Conv2d(1,16,kernel_size=3,stride=1,padding=1),\n",
        "        nn.BatchNorm2d(16),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "        nn.Conv2d(16,32,kernel_size=3,stride=1,padding=1),\n",
        "        nn.BatchNorm2d(32),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "    )\n",
        "```\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jzlGbUbSmEth"
      },
      "source": [
        "## Momentum and other optimizers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o7hyk1fcmIsY"
      },
      "source": [
        "Stochastic gradient descent is the basis of a family of gradient-based algorithms used for deep learning. As we have seen, SGD is not perfetc and there are ways to boost its performance. One of the most fundamental techniques used to better the convergence of SGD is momentum.  The most intuitive way to understand momentum is making an anlogy with the physical world, imagine we have a sphere that goes with a initial speed, if the sphere encounters an uphill it may get stucked because its momentum is not enough for conquering gravity. We can overcome this problem if we add a bigger slope before hill so we can reach our goal at the end.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1x7Rq43khSRRH9heFsxitjXfxBLaNdDTa\" width=600 height=300>\n",
        "\n",
        "In gradient descent, momentum adapts the learning rate in order to accelerate convergence.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1VGj-lN9Q3eL88XDxu1KaW1C7gXj3pggi\" width=600 height=300>\n",
        "\n",
        "In stochastic gradient descent with momentum, information of the previous updating step is used to update the current one:\n",
        "\n",
        "\\begin{equation}\n",
        "\\begin{array}{l}\n",
        "v_{t+1}=\\mu * v_{t}+g_{t+1} \\\\\n",
        "p_{t+1}=p_{t}-\\operatorname{lr} * v_{t+1}\n",
        "\\end{array}\n",
        "\\end{equation}\n",
        "\n",
        "Where $p, g, v \\text{ and } \\mu$ denote the parameters, gradient, velocity and momentum respectively. In this case momentum is an hyperparameter where its value we can set, if it is 0 then we have normal SGD.\n",
        "\n",
        "As said earlier there are many algorithms that can be used for opimizing a Neural Network, In the pytorch [documentation](https://pytorch.org/docs/stable/optim.html#algorithms) you can find more information about them and see what is available for you to solve your problem."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JmNmkaYz0pIq"
      },
      "source": [
        "## Data augmentations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pNwsZz6c0u0H"
      },
      "source": [
        "Generally the data we have may not be enough to achieve generalization, for example we may train a model to recognize images but it may fail is the image is corrupted with noise or is rotated, on the contrary of human perception where we still may recognize the content of an image even if it is disorted.\n",
        "\n",
        "One way to overcome this and achieve greater performance is by performing transformations to the model during training. We may do these transofrmations randomly during the training loop, and by doing this we virtually increase the samples the model see during training, this is why it is called data augmentation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xdh_6Ut-Nbcx"
      },
      "source": [
        "transforms = torchvision.transforms.Compose([\n",
        "            torchvision.transforms.RandomAffine(60),\n",
        "            torchvision.transforms.ToTensor(),\n",
        "]\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yatv9W_dOr7e"
      },
      "source": [
        "train_dataset = datasets.MNIST(root='./',download=True, train=True, transform=transforms)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "T9xkMEeXQ_As",
        "outputId": "5eb64e01-0b59-46fc-995a-2f5568b6a6ac"
      },
      "source": [
        "example = train_dataset[np.random.randint(0,len(train_dataset))]\n",
        "plt.imshow(example[0][0], cmap='gray')\n",
        "plt.title(example[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, '8')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 86
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X_jp4AMxwGNg"
      },
      "source": [
        "# Build a classificator for the Fashion MNIST dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89NQzD2L3LMf"
      },
      "source": [
        "Now we will build a classificator for the fashion MNIST dataset. In the words of the author:\n",
        "\n",
        "\"Fashion-MNIST is a dataset of Zalando's article imagesâ€”consisting of a training set of 60,000 examples and a test set of 10,000 examples. Each example is a 28x28 grayscale image, associated with a label from 10 classes. We intend Fashion-MNIST to serve as a direct drop-in replacement for the original MNIST dataset for benchmarking machine learning algorithms. It shares the same image size and structure of training and testing splits.\"\n",
        "\n",
        "to download the dataset you can clone github repo with the command:\n",
        "```\n",
        "!git clone git@github.com:zalandoresearch/fashion-mnist.git\n",
        "```\n",
        "\n",
        "Or you can use the function in [torchvision](https://pytorch.org/vision/stable/datasets.html#fashion-mnist).\n",
        "\n",
        "This practice will be free, and use of the documentation, working in groups and asking the instructor is encouraged. At the end you should build and test your model which needs to be reproductible in another computer. This problem have the following requirements:\n",
        "\n",
        "1.   You should build two models: One an MLP and the other a CNN.\n",
        "2.   You should use both models on MNIST and Fashion-MNIST.\n",
        "3.   In the MLP model, keeping all the parameters the same, study the effect of increasing the number of layers of the network, use 2, 4 and 8 hidden layers. Make a graph of the accuray respect to the number of layers for both MNIST and Fashion-MNIST.\n",
        "4.   Your can use batchNorm, dropout or any other techique that helps\n",
        " the number of layers and parameters of convolutions is free too.\n",
        "5.   Use both SGD and SGD with momentum optimizer in the MLP an CNN, make a graph comparing the training loss curve with both optimizers for both datasets.\n",
        "6.   Feel free to work in groups, search the web and ask questions but reporting remains individual.\n",
        "7.   The metric to be used is accuracy but you can use as loss function binary cross entropy as before or any other.\n",
        "8.   You must include the code of everything you did which should give all of the results.\n",
        "\n",
        "At the end you should write a report containing the following:\n",
        "\n",
        "*   The training and validations curves of both models for the two datasets.\n",
        "*   The graph comparing the effect of number of layers in the MLP for the two datastes.\n",
        "*   The graph comparing the effect of optimizer descibred in (5).\n",
        "*   A table of accuracy containing rows for the dataset and colummns of model.\n",
        "*   A table summarizing the hyperparameters used for both models.\n",
        "\n",
        "At the end we will make a leaderboard to see who could design a model that achieves the highest accuracy for Fashion-MNIST.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B-twlULlG7Hg"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fldNLzvRHDpX"
      },
      "source": [
        "#MLP for classification\n",
        "\n",
        "def layers(depth,width):\n",
        "  layers = []\n",
        "  layers.append(nn.Linear(28**2,width))\n",
        "  layers.append(nn.ReLU())\n",
        "  for i in range(depth-1):\n",
        "    layers.append(nn.Linear(width,width))\n",
        "    layers.append(nn.ReLU())\n",
        "  layers.append(nn.Linear(width,10))\n",
        "  layers.append(nn.Softmax(dim=1))\n",
        "  return layers\n",
        "\n",
        "class mlp(nn.Module):\n",
        "  def __init__(self, depth, width):\n",
        "    super(mlp,self).__init__()\n",
        "    self.model = nn.Sequential(*layers(depth,width))\n",
        "  def forward(self,x):\n",
        "    x = torch.flatten(x,1)\n",
        "    return self.model(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WYkZxDx8lSWk"
      },
      "source": [
        "#Build your CNN model\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Z4b7mQJIdVr"
      },
      "source": [
        "#Hyperparameters\n",
        "batch_size = 64\n",
        "n_epochs = 20\n",
        "learn_rate = 1e-1\n",
        "depth = 4            # This controls the number of layers of the MLP\n",
        "dataset = 'MNIST'    # MNIST or FMNIST\n",
        "opt = 'sgd_momentum' # sgd or sgd_momentum\n",
        "momentum = 0.9 \n",
        "model_type = 'mlp' #choose betwen mlp or cnn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oP4bCC7DIK2c"
      },
      "source": [
        "#Use GPU if available\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pDL1Hi0MHamU"
      },
      "source": [
        "#Dataset\n",
        "if dataset == 'MNIST':\n",
        "  transform = torchvision.transforms.ToTensor\n",
        "  train_dataset = datasets.MNIST(root='./',download=True, train=True, transform=transform())\n",
        "  test_dataset = datasets.MNIST(root='./', train=False, transform=transform())\n",
        "if dataset == 'FMNIST':\n",
        "  transform = torchvision.transforms.ToTensor\n",
        "  train_dataset = datasets.FashionMNIST(root='./',download=True, train=True, transform=transform())\n",
        "  test_dataset = datasets.FashionMNIST(root='./', train=False, transform=transform())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hn1YwSbhIMDT"
      },
      "source": [
        "if model_type == 'mlp':\n",
        "  model = mlp(depth=depth,width=32).to(device)\n",
        "if model_type == 'cnn':\n",
        "  model = cnn().to(device)\n",
        "print(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QKgG0XzaIfBM"
      },
      "source": [
        "#DataLoaders\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jPH4FkKOVOTl"
      },
      "source": [
        "example = train_dataset[np.random.randint(0,len(train_dataset))]\n",
        "plt.imshow(example[0][0], cmap='gray')\n",
        "plt.title(example[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wq4bWyOxIj_b"
      },
      "source": [
        "#Loss function and Optimizer\n",
        "criterion = nn.BCELoss()\n",
        "if opt == 'sgd':\n",
        "  optimizer = torch.optim.SGD(model.parameters(),lr=learn_rate)\n",
        "if opt == 'sgd_momentum':\n",
        "  optimizer = torch.optim.SGD(model.parameters(),lr=learn_rate, momentum=momentum)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rNIluHIdIpLm"
      },
      "source": [
        "#Training Loop\n",
        "train_loss = []\n",
        "val_loss = []\n",
        "metric = []\n",
        "for epoch in range(n_epochs):  # loop over the dataset multiple times\n",
        "    epoch_loss = 0.0\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(train_loader, 0):\n",
        "        # get the inputs; data is a list of [inputs, labels]\n",
        "        inputs, labels = data\n",
        "        inputs = inputs.to(device)\n",
        "        labels = F.one_hot(labels,10).type_as(inputs)\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "        # print statistics\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        if i % 50 == 49:    # print every 50 mini-batches\n",
        "            print('[%d, %5d] loss: %.3f' %\n",
        "                  (epoch + 1, i + 1, running_loss / 50))\n",
        "            running_loss = 0.0\n",
        "    train_loss.append(epoch_loss/(i+1))\n",
        "    #Evaluation of the trained model\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    epoch_loss = 0.0\n",
        "    print(\"validating...\")\n",
        "    with torch.no_grad():\n",
        "        for i, data in enumerate(test_loader, 0):\n",
        "            inputs, labels = data\n",
        "            inputs = inputs.to(device)\n",
        "            outputs = model(inputs)\n",
        "            predicted = torch.argmax(outputs,dim=1)\n",
        "            loss = criterion(outputs, F.one_hot(labels,10).type_as(inputs))\n",
        "            labels =  labels.type_as(inputs)\n",
        "            total += labels.shape[0]\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            epoch_loss += loss.item()\n",
        "    accuracy = 100 * correct / total\n",
        "    print('Accuracy of the network on the test images: %d %%' % (accuracy))\n",
        "    metric.append(accuracy)\n",
        "    val_loss.append(epoch_loss/(i+1))\n",
        "if model_type=='mlp':\n",
        "  torch.save(metric,model_type + '_metric_'+ dataset +'_'+ opt + '_'+ str(depth)+'_layers.py')\n",
        "  torch.save(train_loss,model_type + '_loss_'+ dataset +'_'+ opt + '_' + str(depth)+'_layers.py')\n",
        "if model_type=='cnn':\n",
        "  torch.save(metric,model_type + '_metric_'+ dataset +'_'+ opt +'.py')\n",
        "  torch.save(train_loss,model_type + '_loss_'+ dataset +'_'+ opt +'.py')\n",
        "print('Finished Training')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jFa1-vwPJEcP"
      },
      "source": [
        "plt.plot(train_loss, label='training')\n",
        "plt.plot(val_loss, label='validation')\n",
        "plt.legend()\n",
        "plt.title('Loss_'+model_type+'_'+dataset)\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.savefig('Loss_'+model_type+'.png')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YMOk_pp3Jnv7"
      },
      "source": [
        "metric1 = torch.load('/content/mlp_metric_MNIST_sgd_2_layers.py')\n",
        "metric2 = torch.load('/content/mlp_metric_MNIST_sgd_4_layers.py')\n",
        "metric3 = torch.load('/content/mlp_metric_MNIST_sgd_6_layers.py')\n",
        "\n",
        "plt.plot(metric1, label='2 layers')\n",
        "plt.plot(metric2, label='4_layers')\n",
        "plt.plot(metric3, label='6_layers')\n",
        "plt.legend()\n",
        "plt.title('Accuracy MNIST')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jMuXt5MwXZnF"
      },
      "source": [
        "metric1 = torch.load('/content/mlp_metric_FMNIST_sgd_2_layers.py')\n",
        "metric2 = torch.load('/content/mlp_metric_FMNIST_sgd_4_layers.py')\n",
        "metric3 = torch.load('/content/mlp_metric_FMNIST_sgd_6_layers.py')\n",
        "\n",
        "plt.plot(metric1, label='2 layers')\n",
        "plt.plot(metric2, label='4_layers')\n",
        "plt.plot(metric3, label='6_layers')\n",
        "plt.legend()\n",
        "plt.title('Accuracy Fashion MNIST')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NvrGKYbPfIlG"
      },
      "source": [
        "metric1 = torch.load('/content/mlp_metric_MNIST_sgd_4_layers.py')\n",
        "metric2 = torch.load('/content/mlp_metric_MNIST_sgd_momentum_4_layers.py')\n",
        "loss1 = torch.load('/content/mlp_loss_MNIST_sgd_4_layers.py')\n",
        "loss2 = torch.load('/content/mlp_loss_MNIST_sgd_momentum_4_layers.py')\n",
        "fig = plt.figure(figsize=(10,5))\n",
        "plt.subplot(1,2,1)\n",
        "plt.plot(metric1, label='sgd')\n",
        "plt.plot(metric2, label='sgd_momentum')\n",
        "plt.legend()\n",
        "plt.title('Accuracy MNIST '+model_type)\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "plt.plot(loss1, label='sgd')\n",
        "plt.plot(loss2, label='sgd_momentum')\n",
        "plt.legend()\n",
        "plt.title('training loss MNIST '+model_type)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "21oLzqxNjgWf"
      },
      "source": [
        "metric1 = torch.load('/content/mlp_metric_FMNIST_sgd_4_layers.py')\n",
        "metric2 = torch.load('/content/mlp_metric_FMNIST_sgd_momentum_4_layers.py')\n",
        "loss1 = torch.load('/content/mlp_loss_FMNIST_sgd_4_layers.py')\n",
        "loss2 = torch.load('/content/mlp_loss_FMNIST_sgd_momentum_4_layers.py')\n",
        "fig = plt.figure(figsize=(10,5))\n",
        "plt.subplot(1,2,1)\n",
        "plt.plot(metric1, label='sgd')\n",
        "plt.plot(metric2, label='sgd_momentum')\n",
        "plt.legend()\n",
        "plt.title('Accuracy Fashion MNIST '+model_type)\n",
        "\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "plt.plot(loss1, label='sgd')\n",
        "plt.plot(loss2, label='sgd_momentum')\n",
        "plt.legend()\n",
        "plt.title('training loss Fashion MNIST '+model_type)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gKM-7feFufqn"
      },
      "source": [
        "metric1 = torch.load('/content/cnn_metric_MNIST_sgd.py')\n",
        "metric2 = torch.load('/content/cnn_metric_MNIST_sgd_momentum.py')\n",
        "loss1 = torch.load('/content/cnn_loss_MNIST_sgd.py')\n",
        "loss2 = torch.load('/content/cnn_loss_MNIST_sgd_momentum.py')\n",
        "fig = plt.figure(figsize=(10,5))\n",
        "plt.subplot(1,2,1)\n",
        "plt.plot(metric1, label='sgd')\n",
        "plt.plot(metric2, label='sgd_momentum')\n",
        "plt.legend()\n",
        "plt.title('Accuracy MNIST '+model_type)\n",
        "\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "plt.plot(loss1, label='sgd')\n",
        "plt.plot(loss2, label='sgd_momentum')\n",
        "plt.legend()\n",
        "plt.title('training loss MNIST '+model_type)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IZ2slv15uAfg"
      },
      "source": [
        "metric1 = torch.load('/content/cnn_metric_FMNIST_sgd.py')\n",
        "metric2 = torch.load('/content/cnn_metric_FMNIST_sgd_momentum.py')\n",
        "loss1 = torch.load('/content/cnn_loss_FMNIST_sgd.py')\n",
        "loss2 = torch.load('/content/cnn_loss_FMNIST_sgd_momentum.py')\n",
        "fig = plt.figure(figsize=(10,5))\n",
        "plt.subplot(1,2,1)\n",
        "plt.plot(metric1, label='sgd')\n",
        "plt.plot(metric2, label='sgd_momentum')\n",
        "plt.legend()\n",
        "plt.title('Accuracy Fashion MNIST '+model_type)\n",
        "\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "plt.plot(loss1, label='sgd')\n",
        "plt.plot(loss2, label='sgd_momentum')\n",
        "plt.legend()\n",
        "plt.title('training loss Fashion MNIST '+model_type)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}