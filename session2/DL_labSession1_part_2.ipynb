{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DL_labSession1_part_2.ipynb",
      "provenance": [],
      "toc_visible": true,
      "mount_file_id": "1uUsrfzZO7DFFiaoJyh-O0W_oCJcrwYbE",
      "authorship_tag": "ABX9TyOdFcak5NpyQzVbh3m+880K",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ferngonzalezp/deep_learning_lab/blob/main/DL_labSession1_part_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_1arUeKub00u"
      },
      "source": [
        "#MNIST Hand-written digits classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TjXHPYa9Of1_"
      },
      "source": [
        "In this part, we will build a Neural Network to classify Handwritten digits. For this part we will use the vanilla neural network, the MLP. We will use the pytorch library for this reason, you can find additional information in the [docs](https://pytorch.org/docs/stable/index.html).\r\n",
        "\r\n",
        "The first part is to import the necessary libraries we will need for this:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pxq9wqpGb77b"
      },
      "source": [
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import torch.nn.functional as F\r\n",
        "import torchvision.datasets as datasets\r\n",
        "import torchvision\r\n",
        "import numpy as np\r\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MPbwKjrIP7eF"
      },
      "source": [
        "The first part is to download the data, we will use the torchvision dataset and download the MNIST dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S0aQst2XQoUm"
      },
      "source": [
        "transform = torchvision.transforms.ToTensor\r\n",
        "train_dataset = datasets.MNIST(root='./',download=True, train=True, transform=transform())\r\n",
        "test_dataset = datasets.MNIST(root='./', train=False, transform=transform())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LnHVPWFhTcy9"
      },
      "source": [
        "Let's explore the dataset to see what it outputs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qmDhfqSIRbhN"
      },
      "source": [
        "example = train_dataset[np.random.randint(0,len(train_dataset))]\r\n",
        "plt.imshow(example[0][0], cmap='gray')\r\n",
        "plt.title(example[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GSe-yWzaTjg1"
      },
      "source": [
        "As we can see we have images with a 28 x 28 resolution and only one channel, the dataset also has an accompanying label to each image and the numbers go from 0 to 9."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "adadk7PrXCtR"
      },
      "source": [
        "____________________\r\n",
        "\r\n",
        "**Builindg a model:**\r\n",
        "\r\n",
        "Pytorch has all the necessary tools for defining a model using the module nn. We will build a Multi Layer Perceptron using linear layers. A common MLP looks like this:\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-nz0QaGJRigX"
      },
      "source": [
        "class mlp(nn.Module):\r\n",
        "  def __init__(self):\r\n",
        "    super(mlp,self).__init__()\r\n",
        "    self.model = nn.Sequential(\r\n",
        "        nn.Linear(28**2,32),\r\n",
        "        nn.Sigmoid(),\r\n",
        "        nn.Linear(32,10),\r\n",
        "        nn.Softmax(dim=1)\r\n",
        "    )\r\n",
        "  def forward(self,x):\r\n",
        "    x = torch.flatten(x,1)\r\n",
        "    return self.model(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qpTFBK3CdC9x"
      },
      "source": [
        "This is a MLP with one hidden layer, the number of hidden layers usually is called \"depth\" and the number of neurons is called \"width\". In this case we have a depth of 1 and a width of 32. Notice that the output has 10 values, these represent each category, and the input represent the number of pixels in the image. In the ``` forward() ``` method of the module we first have to flatten the image in a vector in order for it to be processed by the Linear layers. After the linear layer computation we have to pass the output through a non-linear activation, in this case we use the sigmoid function for the hidden layers because it keeps the otuputs between zero and one and the output is the softmax function that keeps the sum of the given output equal to one, this is because the outputs represents the probability that the image corresponds to a given number.\r\n",
        "\r\n",
        " ______________\r\n",
        "\r\n",
        "We can define the models in a manual way as the previous example, but we can build a model more efficiently by just using building a list recursively and then pass it to the '''nn.Sequential''' module. Try to define a function with arguments **depth** and **width** to build a list of Neural Network layers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1DwXmVDygCtG"
      },
      "source": [
        "def layers(depth,width):\r\n",
        "  layers = []\r\n",
        "  # code for building list of layers\r\n",
        "  #tip1: use the .append() method\r\n",
        "  #tip2: The input and output layers don't change\r\n",
        "  return layers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6K1TiTg2e2Vx"
      },
      "source": [
        "class mlp(nn.Module):\r\n",
        "  def __init__(self, depth, width):\r\n",
        "    super(mlp,self).__init__()\r\n",
        "    self.model = nn.Sequential(*layers(depth,width))\r\n",
        "  def forward(self,x):\r\n",
        "    x = torch.flatten(x,1)\r\n",
        "    return self.model(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PyrOPz8HmavR"
      },
      "source": [
        "Neural networks usually are trained on GPUs, a GPU allows faster calculations of matrix calculations making models parallelize and run faster than on several cpu cores. With the following line of code we detect if there is GPU availaible and create a device instance. For training and using a model on a GPU we simply need to call the ``.to(device)`` method on every torch module and tensors/data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oIbMb8375Vn8"
      },
      "source": [
        "#Use GPU if available\r\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FXcW873Xg-oJ"
      },
      "source": [
        "model = mlp(depth=1,width=32).to(device)\r\n",
        "print(model)\r\n",
        "pytorch_total_params = sum(p.numel() for p in model.parameters())\r\n",
        "print(\"number of parameters in model: %d\"%(pytorch_total_params))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gPjqLcl1nNFT"
      },
      "source": [
        "We need to specify the parameters for training our model, in this case batch size, number of epochs and learning rate."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C-luuEfrmq0T"
      },
      "source": [
        "#Hyperparameters\r\n",
        "batch_size = 100\r\n",
        "n_epochs = 25\r\n",
        "learn_rate = 2e-1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_2wpG_BEnZdr"
      },
      "source": [
        "Pytorch uses dataloaders in order to sample batches from a dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Mp0XD-_mLYF"
      },
      "source": [
        "#DataLoaders\r\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\r\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jPy1YrAnnj2C"
      },
      "source": [
        "The next things we need to define are the loss function and the optimizer. For the optimizer we will use the already studied Stochastic gradient descent, luckily we don't need to code it because pytorch has already a very good implementation along with other [optimizers](https://pytorch.org/docs/stable/optim.html#algorithms). The loss function we will use in this case is the [Binary Cross-Entropy](https://pytorch.org/docs/stable/generated/torch.nn.BCELoss.html#torch.nn.BCELoss). Cross-entropy loss, or log loss, measures the performance of a classification model whose output is a probability value between 0 and 1. Cross-entropy loss increases as the predicted probability diverges from the actual label. So predicting a probability of .012 when the actual observation label is 1 would be bad and result in a high loss value. A perfect model would have a log loss of 0. And for this reason is the we use the softmax function in the output layer, because it will transform the output of the NN in a way that the sum of it will be equal to one. If the model is trained to perfection then the NN will output 1 at the position of the correct label and zero everywhere.\r\n",
        "\r\n",
        "\\\\\r\n",
        "\r\n",
        "So for this reason we need to embed the target labels to a [one-hot](https://en.wikipedia.org/wiki/One-hot) encoding. Pytorch has an integrated function to do that that is ``F.one_hot()``. For example, for the target 5, the one-hot version would be: ``[0,0,0,0,0,1,0,0,0,0]``."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T5fhsK6doR2R"
      },
      "source": [
        "#Loss function and Optimizer\r\n",
        "criterion = nn.BCELoss()\r\n",
        "optimizer = torch.optim.SGD(model.parameters(),lr=learn_rate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NHWeDM4Xo_tQ"
      },
      "source": [
        "#Training Loop\r\n",
        "train_loss = []\r\n",
        "val_loss = []\r\n",
        "for epoch in range(n_epochs):  # loop over the dataset multiple times\r\n",
        "    epoch_loss = 0.0\r\n",
        "    running_loss = 0.0\r\n",
        "    for i, data in enumerate(train_loader, 0):\r\n",
        "        # get the inputs; data is a list of [inputs, labels]\r\n",
        "        inputs, labels = data\r\n",
        "        inputs = inputs.to(device)\r\n",
        "        labels = F.one_hot(labels,10).type_as(inputs)\r\n",
        "        # zero the parameter gradients\r\n",
        "        optimizer.zero_grad()\r\n",
        "\r\n",
        "        # forward + backward + optimize\r\n",
        "        outputs = model(inputs)\r\n",
        "        loss = criterion(outputs, labels)\r\n",
        "        loss.backward()\r\n",
        "        optimizer.step()\r\n",
        "        epoch_loss += loss.item()\r\n",
        "        # print statistics\r\n",
        "        running_loss += loss.item()\r\n",
        "\r\n",
        "        if i % 50 == 49:    # print every 50 mini-batches\r\n",
        "            print('[%d, %5d] loss: %.3f' %\r\n",
        "                  (epoch + 1, i + 1, running_loss / 50))\r\n",
        "            running_loss = 0.0\r\n",
        "    train_loss.append(epoch_loss/(i+1))\r\n",
        "    #Evaluation of the trained model\r\n",
        "    correct = 0\r\n",
        "    total = 0\r\n",
        "    epoch_loss = 0.0\r\n",
        "    print(\"validating...\")\r\n",
        "    with torch.no_grad():\r\n",
        "        for i, data in enumerate(test_loader, 0):\r\n",
        "            inputs, labels = data\r\n",
        "            inputs = inputs.to(device)\r\n",
        "            outputs = model(inputs)\r\n",
        "            predicted = torch.argmax(outputs,dim=1)\r\n",
        "            loss = criterion(outputs, F.one_hot(labels,10).type_as(inputs))\r\n",
        "            labels =  labels.type_as(inputs)\r\n",
        "            total += labels.shape[0]\r\n",
        "            correct += (predicted == labels).sum().item()\r\n",
        "            epoch_loss += loss.item()\r\n",
        "    print('Accuracy of the network on the test images: %d %%' % (\r\n",
        "        100 * correct / total))\r\n",
        "    val_loss.append(epoch_loss/(i+1))\r\n",
        "\r\n",
        "print('Finished Training')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PVRIjEJtjcgQ"
      },
      "source": [
        "plt.plot(train_loss, label='training')\r\n",
        "plt.plot(val_loss, label='validation')\r\n",
        "plt.legend()\r\n",
        "plt.title('Loss')\r\n",
        "plt.xlabel('Epochs')\r\n",
        "plt.ylabel('Loss')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C44uK2Cze-Eg"
      },
      "source": [
        "Now, we want to test our trained model. For this task we use Accurracy as performance metric. Accuracy is as simples as:\r\n",
        "\r\n",
        "\\begin{equation}\r\n",
        "  Accuracy = \\frac{Number \\ of \\ correct \\ predictions}{Total \\ number \\ of \\ predictions}\r\n",
        "\\end{equation}\r\n",
        "\r\n",
        "Although it is woth to note that accuracy is not a reliable measure in real life but for this case it is good enough. Try to think in which cases accuracy fails to correcly measure the performance of a model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z8bvFBK8WSlM"
      },
      "source": [
        "#Evaluation of the trained model\r\n",
        "correct = 0\r\n",
        "total = 0\r\n",
        "with torch.no_grad():\r\n",
        "    for i, data in enumerate(test_loader, 0):\r\n",
        "        inputs, labels = data\r\n",
        "        inputs = inputs.to(device)\r\n",
        "        labels =  labels.type_as(inputs)\r\n",
        "        outputs = model(inputs)\r\n",
        "        predicted = torch.argmax(outputs,dim=1)\r\n",
        "        total += labels.shape[0]\r\n",
        "        correct += (predicted == labels).sum().item()\r\n",
        "\r\n",
        "print('Accuracy of the network on the test images: %d %%' % (\r\n",
        "    100 * correct / total))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yxzf7-5heRxn"
      },
      "source": [
        "So we got a 93% prediction accuracy with this model with just one hidden layer, do you think you can improve this results? Below we plot some test samples along with their predicted label."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jf5XyBiHZVfl"
      },
      "source": [
        "from math import ceil\r\n",
        "n_samples = 6\r\n",
        "id = np.random.randint(0,len(test_dataset),n_samples)\r\n",
        "rows = ceil(n_samples/3)\r\n",
        "plt.figure(figsize=(rows*5,10))\r\n",
        "for i in range(len(id)):\r\n",
        "  images = test_dataset[id[i]][0].to(device)\r\n",
        "  pred_labels = torch.argmax(model(images),dim=1)[0]\r\n",
        "  plt.subplot(rows,3,i+1)\r\n",
        "  plt.imshow(images[0].cpu(), cmap='gray')\r\n",
        "  plt.title('prediction: %d'%(pred_labels))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
