{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TP1-ML for science and engineering.ipynb",
      "provenance": [],
      "toc_visible": true,
      "mount_file_id": "1uUsrfzZO7DFFiaoJyh-O0W_oCJcrwYbE",
      "authorship_tag": "ABX9TyMfDqsCUpk/e6+OQnJm/ntV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ferngonzalezp/deep_learning_lab/blob/main/2022/TP1_ML_for_science_and_engineering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wGrRoyk-ARTT"
      },
      "source": [
        "#Introduction to Deep Learning with Python: session 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qLUXc-MIAlg-"
      },
      "source": [
        "Welcome to this introduction of Machine Learning with Python. This lab sessions are designed to give the student a practical introduction to the world of Machine Learning along with some theory. The goal of this course is to get a hands-on introduction to Machine learning and be able to use these tools to solve practicals problems in science and engineering. Nowadays, data is enabling new discoveries in many fields of informatics and science, is for great importance for the scientists and engineers of today to be able to understand anto be skeptical of Machine Learning and see it more of a tool in the arsenal with its downsides and benefits rather than alchemy or a fashion.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L9o9ahf-Fd_b"
      },
      "source": [
        "#Inital Motivation for Machine Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WGe9ksLQHouG"
      },
      "source": [
        "Machine Learning has gained recent Interest in the last years, and for good reason. It has allowed scientist and engineers to achieve tasks with greater degree of accuracy that couldn't be achieved before with other methods. Right now deep learning has been used extensively in the following tasks in which is the best method there is right now:\n",
        "\n",
        "\n",
        "* Natural language processing (NLP): Answering questions; speech recognition; summarizing documents; classifying documents; finding names, dates, etc. in documents; searching for articles mentioning a concept\n",
        "*   Computer vision:: Satellite and drone imagery interpretation (e.g., for disaster resilience); face recognition; image captioning; reading traffic signs; locating pedestrians and vehicles in autonomous vehicles\n",
        "*    Medicine:: Finding anomalies in radiology images, including CT, MRI, and X-ray images; counting features in pathology slides; measuring features in ultrasounds; diagnosing diabetic retinopathy\n",
        "*    Biology:: Folding proteins; classifying proteins; many genomics tasks, such as tumor-normal sequencing and classifying clinically actionable genetic mutations; cell classification; analyzing protein/protein interactions\n",
        "*    Image generation:: Colorizing images; increasing image resolution; removing noise from images; converting images to art in the style of famous artists\n",
        "*    Recommendation systems:: Web search; product recommendations; home page layout\n",
        "*    Playing games:: Chess, Go, most Atari video games, and many real-time strategy games\n",
        "*    Robotics:: Handling objects that are challenging to locate (e.g., transparent, shiny, lacking texture) or hard to pick up\n",
        "*    Other applications:: Financial and logistical forecasting, text to speech, and much more...\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1aC1TZuNXtlV"
      },
      "source": [
        "# What is Machine Learning?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YVnl24Aw2AFi"
      },
      "source": [
        "We'll let machine learning respond that question. For that reason we wrote a custom prompt in this [web app](https://app.inferkit.com/demo) that said \"**what is machine learning? Machine learning is**\" and got this answer:\n",
        "\n",
        "\\\\\n",
        "`What is machine learning? Machine learning is a form of artificial intelligence where computers are given experiences and the computer \"learns\" from the experience and improves its own future predictions. Essentially, it lets the computer draw conclusions from the patterns it has observed in the past. This is useful for all sorts of reasons, such as automatic speech recognition (for example) or weather forecasts. It is an area of computing that is set to dominate the future and has been given a lot of attention in the tech industry.`\n",
        "\n",
        "\\\\\n",
        "That is a very good description. As the statement implies, machine learning is basically a set of methods to program computers in order that they can learn from examples. For that model to be able to reproduce a text as the previous ones it required lots and lots of text data and adjust itself internally based on a score of some sort, we'll later talk about that. In this picture is illustrated the building blocks of a machine learning model.\n",
        "\n",
        "\\\\\n",
        "<img src='https://drive.google.com/uc?id=1JIpwDf4ZFxcLkTwYoY-ij9UBDsbkeoz-'>\n",
        "\n",
        "Deep learning is the field inside of machine learning that uses Neural Networks as its principal building block for models. Deep learning also introduces a paradigm shift on how machine learning is done: In traditional machine learning, the input data has to be pre-processed extensively and the features to be learned by the algorithm has to be manually defined. In Deep Learning, these features or representations are learned automatically by the model, in exchange, deep learning models tendo to need more data and computational resources in most cases.\n",
        "\n",
        "\\\\\n",
        "<img src='https://drive.google.com/uc?id=1K3zwaC6uTlfu4n_YdnWTtrD1lIzFEPxm' width=\"500\" height=\"300\">\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61ZO3kcALfnc"
      },
      "source": [
        "## How these models learn?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TQSGcM5gLmO8"
      },
      "source": [
        "After we feed the model data and it outputs a prediction, we need to pass that value to a score we want to optimize, this score is usually called the \"loss\" of a model, and the training process is basically the optimization of this value.\n",
        "\n",
        "\\\\\n",
        "\n",
        "The optimization process for Neural Networks is based on [gradient descent](https://en.wikipedia.org/wiki/Gradient_descent).\n",
        "\n",
        "\\\\\n",
        "\n",
        "<img src=\"https://i.ytimg.com/vi/b4Vyma9wPHo/maxresdefault.jpg\" width=500 height=300>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C3bEo1SwWSyj"
      },
      "source": [
        "________________________\n",
        "\n",
        "Now, we will see a gradient descent eaxmple using python. Using python and numpy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cSZjOIueIp-Z"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def gradient_descent(\n",
        "\n",
        "    gradient, start, learn_rate, n_iter=50, tolerance=1e-06\n",
        "\n",
        "):\n",
        "\n",
        "    vector = start\n",
        "\n",
        "    for _ in range(n_iter):\n",
        "\n",
        "        diff = -learn_rate * gradient(vector)\n",
        "\n",
        "        if np.all(np.abs(diff) <= tolerance):\n",
        "\n",
        "            break\n",
        "\n",
        "        vector += diff\n",
        "\n",
        "    return vector"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-KmdnF_YXoHc"
      },
      "source": [
        "\n",
        "*    **gradient** is the function or any Python callable object that takes a vector and returns the gradient of the function youâ€™re trying to minimize.\n",
        "*    **start** is the point where the algorithm starts its search, given as a sequence (tuple, list, NumPy array, and so on) or scalar (in the case of a one-dimensional problem).\n",
        "*    **learn_rate** is the learning rate that controls the magnitude of the vector update.\n",
        "*   **n_iter** is the number of iterations.\n",
        "\n",
        "\\\\\n",
        "\n",
        "We will use this algorithm to find the minimum of a simple convex function:\n",
        "\n",
        "$y = x^2$\n",
        "\n",
        "which we know it's gradient is:\n",
        "\n",
        "$y' = 2x$\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NNpwvBL7XOmE"
      },
      "source": [
        "y_prime = lambda x: 2*x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JMM1StaReFi8"
      },
      "source": [
        "Here, we use gradient descent to find the minimum of a function. And plot the different steps it takes until it finds the minimum. Try to adjust the learning rate to see what happens."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vTBUrCk_atBh"
      },
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#Parameters for Gradient descent\n",
        "start = 10\n",
        "learn_rate = 0.2\n",
        "tol = 1e-6\n",
        "n_steps = 50\n",
        "\n",
        "#Plot of the function\n",
        "x = np.linspace(-10,10)\n",
        "y = lambda x: x**2\n",
        "plt.plot(x,y(x))\n",
        "\n",
        "#The optimization loop, we do it in such a way to obtain the result in each step.\n",
        "results =[start]\n",
        "for step in range(n_steps):\n",
        "  \n",
        "  min = gradient_descent(y_prime,start,learn_rate,1)\n",
        "  results.append(min)\n",
        "  if np.abs(start-min) < tol:\n",
        "    break\n",
        "  start = min\n",
        "\n",
        "results = np.array(results)\n",
        "plt.plot(results, y(results), '-*g')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AmJdKNpNfG_T"
      },
      "source": [
        "Try to change the function to a non convex one and it's gradient to see how it behaves!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V7_J98A8feRz"
      },
      "source": [
        "_________________________________________\n",
        "Gradient Descent is a simple and fast way to find a minimum, but when the number of parameters of a function increases and when there is non-convexity then the algorithm requires some modifications to work properly."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise: Find Optimal Dimensions\n",
        "\n",
        "We want to fabricate an aluminum can with cylindrical shape with capacity for 1L. What the dimensions should be for minimizing the mass?\n",
        "\n",
        "First, formulate the optimization problem and find the optimal values using gradient descent, assume the thickness of the can is fixed."
      ],
      "metadata": {
        "id": "zm-dWW01bmI-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your solution below"
      ],
      "metadata": {
        "id": "-_lMBfl0dOe3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3NOTLZUz6A6E"
      },
      "source": [
        "## Stochastic Gradient Descent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5_AbPLO46EVb"
      },
      "source": [
        "Stochastic gradient descent algorithms are a modification of gradient descent. In stochastic gradient descent, you calculate the gradient using just a random small part of the observations instead of all of them. In some cases, this approach can reduce computation time.\n",
        "\n",
        "Online stochastic gradient descent is a variant of stochastic gradient descent in which you estimate the gradient of the cost function for each observation and update the decision variables accordingly. This can help you find the global minimum, especially if the objective function is convex.\n",
        "\n",
        "Batch stochastic gradient descent is somewhere between ordinary gradient descent and the online method. The gradients are calculated and the decision variables are updated iteratively with subsets of all observations, called minibatches. This variant is very popular for training neural networks.\n",
        "\n",
        "You can imagine the online algorithm as a special kind of batch algorithm in which each minibatch has only one observation. Classical gradient descent is another special case in which thereâ€™s only one batch containing all observations.\n",
        "\n",
        "From now on we will refer to the minibatch stocasthic gradient descent as stochastic gradient descent (SGD)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iv5l9QLn7idA"
      },
      "source": [
        "# Linear Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qxpGtS0oZkiV"
      },
      "source": [
        "#### Simple Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cW2jOofWZo3n"
      },
      "source": [
        "We will start by a [simple Linear regression problem](https://en.wikipedia.org/wiki/Simple_linear_regression). In this problem we want to fit a distribution of points in 2D space with a curve. This curve can be linear, a polynomial of any order or even an exponential; the selection of curve depends on the nature of the data we have at hand, and to some extent this represents a limititation of doing regression in this fashion, we need certain prior information of the data befor applying the model, as opposed to the use of neural networks where we can learn that this distribution (but let's leave this to a further topic).\n",
        "\n",
        "\\\\\n",
        "\n",
        "First, we will do Linear regression in a classical way using the Least squares method. In linear regresion we want to the determine the slope and intercept (weight and bias) of a line equation:\n",
        "\n",
        "\\begin{equation}\n",
        "  \\hat{y} = b + w x\n",
        "\\end{equation}\n",
        "\n",
        "\\\\\n",
        "\n",
        "In the least squares method, we want to minimize the squared error between predictions and observations:\n",
        "\n",
        "\\begin{equation}\n",
        "  error = \\frac{1}{n} \\sum^n{(y-\\hat{y})^2}\n",
        "\\end{equation}\n",
        "\n",
        "\\\\\n",
        "\n",
        "We then can find the weight and bias just by taking the derivative of the error w.r.t. the parameters of our curve and equal to zero. We now are going to derive the formula for finding these values but first we will reformulate or curve equation in a matrix way:\n",
        "\n",
        "\\begin{equation}\n",
        "  \\hat{y} = \\begin{bmatrix}1 & x\\end{bmatrix} . \\begin{bmatrix}b\\\\w\\end{bmatrix}\n",
        "\\end{equation}\n",
        " \\\\\n",
        "Now this reduces to a linear system, that can be rewritten as  $\\hat{y}=X\\theta$, where $\\theta$ is the parameters vector of our model and X is the set of independent variables. We can now rewrite the error as:\n",
        "\n",
        "\n",
        "\\begin{equation}\n",
        "  error = (Y-X\\theta)^2\\\\\n",
        "  error = (Y-X\\theta)^T(Y-X\\theta)\\\\\n",
        "  error = Y^TY - \\theta^TX^TY - Y^TX\\theta + \\theta^TX^TX\\theta\\\\\n",
        "  error = Y^TY - 2\\theta^TX^TY + \\theta^TX^TX\\theta\n",
        "\\end{equation}\n",
        "\n",
        "\\\\\n",
        "\n",
        "This reduction is possible because $\\theta^TX^TY = Y^TX\\theta$, which are scalars and their transpose is the same scalar:\n",
        "\n",
        "\\begin{equation}\n",
        "  (\\theta^TX^TY)^T = (X^TY)^T\\theta = Y^TX\\theta\n",
        "\\end{equation}\n",
        "\n",
        "\\\\\n",
        "\n",
        "The the derivative of the error w.r.t. the parameters is:\n",
        "\n",
        "\\begin{equation}\n",
        "  \\partial{error}/\\partial{\\theta} = -2X^TY + X^TX\\theta + \\theta^TX^TX = -2X^TY + 2X^TX\\theta\n",
        "\\end{equation}\n",
        "\n",
        "\\\\\n",
        "\n",
        "Equalizing the derivative to zero when then have:\n",
        "\n",
        "\\begin{equation}\n",
        "  X^TY = X^TX\\theta\\\\\n",
        "  \\theta = (X^TX)^{-1}X^TY\n",
        "\\end{equation}\n",
        "\n",
        "\\\\\n",
        "\n",
        "Now we have a general formula for fitting the parameters, for any linear function!\n",
        "\n",
        "Now let's solve a simple problems, first we need data to fit. For this reason we will generate some points from a cuadractic function, but we will add some random noise:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QNb6Eg9qKyVa"
      },
      "source": [
        "n_samples = 20\n",
        "X = np.sort(np.random.uniform(0,10,(n_samples)))\n",
        "y = lambda x: x**2 + np.random.uniform(-10,10,(len(x)))\n",
        "Y = y(X)\n",
        "plt.scatter(X,Y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e0zQLywRqDgg"
      },
      "source": [
        "Here we make a function to calculate the vector of parameters based on the previous formula:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tJawFKr-NnYM"
      },
      "source": [
        "def lssr(Y, X):\n",
        "  X = np.vstack((np.ones(np.size(X,axis=-1)),X)).transpose()\n",
        "  B = np.matmul(X.transpose(),X)\n",
        "  B = np.matmul(np.linalg.inv(B),X.transpose())\n",
        "  return np.matmul(B,Y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GvNnotqyqOH-"
      },
      "source": [
        "Determine the parameters of our curve:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i4ThS2zOR1cH"
      },
      "source": [
        "Theta = lssr(Y,X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OW9C4gsTqZv4"
      },
      "source": [
        "Now visualize the fitted line and the observable points."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RqMPf2EwULd4"
      },
      "source": [
        "plt.scatter(X,Y)\n",
        "fit = Theta[0] + Theta[1]*X\n",
        "plt.plot(X,fit,'r')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "frbv5NKhqi1H"
      },
      "source": [
        "Now we are goind to try to fit the points but with a cuadratic function: $y = b + w_1x + w_2x^2$. Use the function we created before to obtain the parameters of this curve."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gq0PqeH0Ul4N"
      },
      "source": [
        "x2 = X**2\n",
        "X = np.vstack((X,x2))\n",
        "Theta_cuadratic = lssr(Y,X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2lsItQgrrASN"
      },
      "source": [
        "Now plot the results, is this model better suited to the data?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bQd8bV9ZVnMl"
      },
      "source": [
        "plt.scatter(X[0],Y)\n",
        "fit = Theta_cuadratic[0] + Theta_cuadratic[1]*X[0] + Theta_cuadratic[2]*X[1]\n",
        "plt.plot(X[0],fit,'r')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MM4x2utQrO9R"
      },
      "source": [
        "Now we are going to use gradient descent, to solve this same problem. First we have to create a function ta updates the coefficients based on a gradient descent step. Remember that we are trying to minimize de error function, in gradient descent we don't need an analytic formula of the gradient so we can simply take as gradient: \n",
        "\\begin{equation}\n",
        "\\partial{error}/\\partial{\\theta} = \\frac{1}{n}\\sum^n 2(\\hat{Y}-Y)X\n",
        "\\end{equation}\n",
        "\\\\\n",
        "being $n$ the number of observations. Now we write the function:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4FDF7dFfX05Z"
      },
      "source": [
        "def sgd_update(X, Y, Theta, l_rate):\n",
        "      X = np.vstack((np.ones(np.size(X,axis=-1)),X)).transpose()\n",
        "      Yhat = np.matmul(X,Theta)\n",
        "      grad_error = np.mean(np.einsum('i,ij->ij',2*(Yhat-Y),X), axis = 0)\n",
        "      Theta  = Theta - l_rate * grad_error\n",
        "      return Theta"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lZsA8ix5tXo4"
      },
      "source": [
        "In this part we find the optimun using SGD:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QtGjCwq2amnU"
      },
      "source": [
        "Theta_sgd = np.zeros(2)\n",
        "n_epochs = 2000\n",
        "lr = 0.01\n",
        "for i in range(n_epochs):\n",
        "  y = sgd_update(X[0],Y,Theta_sgd,lr)\n",
        "  diff = np.abs(np.sum(y-Theta_sgd))\n",
        "  Theta_sgd = y\n",
        "  if diff < 1e-6:\n",
        "    break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pVN6t7gltcIg"
      },
      "source": [
        "Now we plot the results:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OZZ40moLcOUl"
      },
      "source": [
        "plt.scatter(X[0],Y)\n",
        "fit = Theta_sgd[0] + Theta_sgd[1]*X[0]\n",
        "plt.plot(X[0],fit,'r')\n",
        "error_linear = np.mean((Y - fit)**2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y4pAme8b7i7o"
      },
      "source": [
        "print(error_linear)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D2ealy5BtfWp"
      },
      "source": [
        "We compare the results obtained with the analytical way and using SGD:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ENxzPbiRdr8f"
      },
      "source": [
        "print('result with SGD:')\n",
        "print(Theta_sgd)\n",
        "print('result in analytical way:')\n",
        "print(Theta)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w8VHLPCSt8v_"
      },
      "source": [
        "Now we are going to fit a line with some added gaussian noise:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zx3gL5OohLKV"
      },
      "source": [
        "n_samples = 50\n",
        "X = np.sort(np.random.uniform(-10,10,(n_samples)))\n",
        "y = lambda x: x + np.random.normal(0,1,n_samples)\n",
        "Y = y(X)\n",
        "plt.scatter(X,Y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AGmVj4En8SlJ"
      },
      "source": [
        "coef_sgd = np.zeros(2)\n",
        "n_epochs = 2000\n",
        "lr = 0.01\n",
        "for i in range(n_epochs):\n",
        "  y = sgd_update(X,Y,coef_sgd,lr)\n",
        "  diff = np.abs(np.sum(y-coef_sgd))\n",
        "  coef_sgd = y\n",
        "  if diff < 1e-6:\n",
        "    break\n",
        "\n",
        "plt.scatter(X,Y)\n",
        "fit = coef_sgd[0] + coef_sgd[1]*X\n",
        "plt.plot(X,fit,'r')\n",
        "error_linear = np.mean((Y - fit)**2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eKZmOrCo8dgu"
      },
      "source": [
        "print(error_linear)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gn6z78e6uDRq"
      },
      "source": [
        "But this time we are going to use a linear polynomial or third order and use SGD:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mwz6kQEXhhbQ"
      },
      "source": [
        "coef_sgd = np.zeros(4)\n",
        "#Based on what you have learned so far, fit the points using a third order polynomial: y = b + w1*x + w2*x**2 + w3*x**3 "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1jKtAbq6uP-G"
      },
      "source": [
        "Now we plot the results, what happended?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KrcOPT_ViRLP"
      },
      "source": [
        "#plot the points and the fitted polynomial\n",
        "#Also, calculate de Mean Squared error\n",
        "error_cubic = "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0jQC-AOI7KCW"
      },
      "source": [
        "print(error_cubic)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4nRihBY0uchG"
      },
      "source": [
        "This phenomenon is called [overfitting](https://en.wikipedia.org/wiki/Overfitting). Here it happened because our model has more parameters than need for the data, it fitted exactly some points of the distribution but in certain areas the error may be to high, resulting in a higher MSE."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CfFg4-sYZaFz"
      },
      "source": [
        "#### Multi_Dimensional Linear Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aFR923Ph8Hwk"
      },
      "source": [
        "In this part we will try to solve a [Linear regression](https://en.wikipedia.org/wiki/Linear_regression) problem using SGD programmed from scratch.\n",
        "After we develop our linear regression algorithm with stochastic gradient descent, we will use it to model the wine quality dataset.\n",
        "\n",
        "This dataset is comprised of the details of 4,898 white wines including measurements like acidity and pH. The goal is to use these objective measures to predict the wine quality on a scale between 0 and 10.\n",
        "\n",
        "Each columm of the dataset represent the following attributes:\n",
        "\n",
        "Attribute information:\n",
        "\n",
        "   Input variables (based on physicochemical tests):\n",
        "   1. fixed acidity\n",
        "   2. volatile acidity\n",
        "   3. citric acid\n",
        "   4. residual sugar\n",
        "   5. chlorides\n",
        "   6. free sulfur dioxide\n",
        "   7. total sulfur dioxide\n",
        "   8. density\n",
        "   9. pH\n",
        "   10. sulphates\n",
        "   11. alcohol\n",
        "   Output variable (based on sensory data): \n",
        "   12. quality (score between 0 and 10)\n",
        "\n",
        "Click [here](https://drive.google.com/file/d/1KKvWK7pmDtJXN5zEA7w0Xje-qbP2FrJI/view?usp=sharing) to download the dataset and the [description](https://drive.google.com/file/d/1dhXvJgR5TRWCk69VTpOznrlyIyTfG3JE/view?usp=sharing).\n",
        "\n",
        "the function we are trying to fit with this method it is:\n",
        "\n",
        "\\begin{equation}\n",
        "  \\hat{Y} = b_0 + W^T X = X\\theta\n",
        "\\end{equation}\n",
        "\n",
        "Where b_0 is a bias, W is the weight vector and X is the vector of independent variables.\n",
        "\n",
        "\\\\\n",
        "\n",
        "In this case we will use mini-batches in stochastic gradient descent for calculating the gradients and updating, the motivation for this is that by doing it this way we are dealing with matrixes of reduced size. The forumula for mini-batch of SGD changes to:\n",
        "\n",
        "\\begin{equation}\n",
        "  \\theta_k = \\theta_{k-1} - lr * \\frac{1}{b}\\sum_1^b 2(\\hat{Y}-Y)X \\\\\n",
        "  b < size \\ of \\ the \\ dataset\n",
        "\\end{equation}\n",
        "\n",
        "With this two terms arise:\n",
        "\n",
        "* Iteration: Is a SGD update step with a batch.\n",
        "* Epoch: an epoch completes when we iterate throug the whole training dataset. For better performance, the batches elements are picked randomly from the dataset, this helps with convergence.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N-O4xSxBf5VU"
      },
      "source": [
        "# Linear Regression With Stochastic Gradient Descent for Wine Quality\n",
        "from random import seed\n",
        "from random import randrange\n",
        "from csv import reader\n",
        "from math import sqrt\n",
        "from math import ceil\n",
        "from numpy import genfromtxt\n",
        "from random import shuffle\n",
        "\n",
        "# Find the min and max values for each column\n",
        "def dataset_minmax(dataset):\n",
        "\tminmax = list()\n",
        "\tfor i in range(len(dataset[0])):\n",
        "\t\tcol_values = dataset[:,i]\n",
        "\t\tvalue_min = col_values.min()\n",
        "\t\tvalue_max = col_values.max()\n",
        "\t\tminmax.append([value_min, value_max])\n",
        "\treturn minmax\n",
        " \n",
        "# Rescale dataset columns to the range 0-1\n",
        "def normalize_dataset(dataset, minmax):\n",
        "  for i in range(len(dataset[0])):\n",
        "\t\t\tdataset[:,i] = (dataset[:,i] - minmax[i][0]) / (minmax[i][1] - minmax[i][0])\n",
        "  return dataset\n",
        "\n",
        "# Make a prediction with coefficients\n",
        "def predict(batch, coefficients):\n",
        "    yhat = np.matmul(batch[:,:-1],coefficients[1:]) + coefficients[0]\n",
        "    return yhat\n",
        "\n",
        "# Estimate linear regression coefficients using stochastic gradient descent\n",
        "def sgd_update(batch, Theta, l_rate):\n",
        "      X = batch[:,:-1].transpose()\n",
        "      Y = batch[:,-1]\n",
        "      X = np.vstack((np.ones(np.size(X,axis=-1)),X)).transpose()\n",
        "      Yhat = np.matmul(X,Theta)\n",
        "      grad_error = np.mean(np.einsum('i,ij->ij',2*(Yhat-Y),X), axis = 0)\n",
        "      Theta  = Theta - l_rate * grad_error\n",
        "      return Theta"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LhReyNBDQVGj"
      },
      "source": [
        "#Evaluation Metric\n",
        "def mse_metric(actual, predicted):\n",
        "  return np.mean((actual-predicted)**2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6FZk1re-Qtvc"
      },
      "source": [
        "# Linear Regression on wine quality dataset\n",
        "seed(1)\n",
        "# load and prepare data\n",
        "filename = 'winequality-white.csv'\n",
        "#dataset = load_csv(filename)\n",
        "dataset = genfromtxt(filename, delimiter=';')\n",
        "# normalize\n",
        "minmax = dataset_minmax(dataset)\n",
        "dataset = normalize_dataset(dataset, minmax)\n",
        "train_data = dataset[:4000]\n",
        "test_data = dataset[4000:]\n",
        "#Training paramateres\n",
        "n_epochs = 500\n",
        "batch_size = 500\n",
        "l_rate = 0.1\n",
        "#Initialize Coefficients with zeros\n",
        "coef = np.zeros((len(dataset[0])))\n",
        "#indexes of rows\n",
        "idx = list(range(len(train_data[:,0])))\n",
        "n_iterations = ceil(len(train_data[:,0])/batch_size)\n",
        "#training loop\n",
        "loss = []\n",
        "for i in range(n_epochs):\n",
        "  shuffle(idx)\n",
        "  for j in range(n_iterations):\n",
        "    if j==n_iterations-1:\n",
        "      batch = train_data[idx[j*batch_size:]]\n",
        "    else:\n",
        "      batch = train_data[idx[j*batch_size:(j+1)*batch_size]]\n",
        "    coef = sgd_update(batch, coef, l_rate)\n",
        "    prediction = predict(batch, coef)\n",
        "    mse = mse_metric(batch[:,-1],prediction)\n",
        "    loss.append(mse)\n",
        "    if j%50 == 0 :\n",
        "      print(\"epoch: %d Iteration: %d/%d MSE: %.4f \\n\" % ((i+1), (j+1), n_iterations, mse))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JebM9gwcnvjl"
      },
      "source": [
        "plt.plot(loss)\n",
        "plt.title(\"MSE\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6JgD2NpEOO86"
      },
      "source": [
        "Now we test the coefficients with the test data we didn't used for training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "beKnhDNeON1q"
      },
      "source": [
        "test_predictions = predict(test_data,coef)\n",
        "print(mse_metric(test_data[:,-1],test_predictions))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NpKVYeb8-cC9"
      },
      "source": [
        "idxs = np.random.randint(0,np.size(test_data,0),5)\n",
        "print('Wine Quality:')\n",
        "print(test_data[idxs,-1]*(minmax[-1][1]-minmax[-1][0]) + minmax[-1][0])\n",
        "print('Predicted Wine Quality:')\n",
        "print(test_predictions[idxs]*(minmax[-1][1]-minmax[-1][0]) + minmax[-1][0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wjgBgzJKZiHn"
      },
      "source": [
        "We obtained a fit for the data with a Mean squared error of 0.014, try changing the batch size and learning rate to see what effect it gives to the optimization process. Maybe the advantage of SGD cannot be seen in this problem but as dimensionality increases it becomes a really useful tool for training Deep learning models."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise: Linear Regression\n",
        "\n",
        "given the following relation:\n",
        "\n",
        "\n",
        "$$ k = \\frac{k_{max}c^2}{c_s + c}$$\n",
        "\n",
        "where $c_s$ and $k_{max}$ are parameters. Linearize the equation and given the data below use linear regression with gradient descent for finding $c_s$ and $k_{max}$. Use the parameters to find k when c = 6 mg/L.\n",
        "\n",
        "![image.png](data:image/png;base64,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)"
      ],
      "metadata": {
        "id": "PWv-jOajhiRS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ghTiZQVddZ8O"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}