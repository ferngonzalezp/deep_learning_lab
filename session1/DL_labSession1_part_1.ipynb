{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DL_labSession1_part_1.ipynb",
      "provenance": [],
      "toc_visible": true,
      "mount_file_id": "1uUsrfzZO7DFFiaoJyh-O0W_oCJcrwYbE",
      "authorship_tag": "ABX9TyPY222H8i4SeHnbUTKpJRIt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ferngonzalezp/deep_learning_lab/blob/main/DL_labSession1_part_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wGrRoyk-ARTT"
      },
      "source": [
        "#Introduction to Deep Learning with Python: session 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qLUXc-MIAlg-"
      },
      "source": [
        "Welcome to this introduction of Deep Learning with Python. This lab sessions are designed to give the student a practical introduction to the world of Deep Learning along with some theory. After the course completion you will have learned:\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "*   What is Deep Learning.\r\n",
        "*   What is a Nerual Network.\r\n",
        "*   How Nerual Networks are trained, basics of Stochastic Gradient descent.\r\n",
        "*   Handwritten digit Classification.\r\n",
        "*   Convolutional Neural Networks, uses in Computer Vision.\r\n",
        "*   Detection of diseases in medical images.\r\n",
        "*   Recurrent Neural Networks.\r\n",
        "*   Examples in Natural Language processing.\r\n",
        "*   How to train and build your own models.\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "---\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L9o9ahf-Fd_b"
      },
      "source": [
        "#Inital Motivation for Deep Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WGe9ksLQHouG"
      },
      "source": [
        "Deep Learning has gained recent Interest in the last years, and for good reason. It has allowed scientist and engineers to achieve tasks with greater degree of accuracy that couldn't be achieved before with other methods. Right now deep learning has been used extensively in the following tasks in which is the best method there is right now:\r\n",
        "\r\n",
        "\r\n",
        "* Natural language processing (NLP): Answering questions; speech recognition; summarizing documents; classifying documents; finding names, dates, etc. in documents; searching for articles mentioning a concept\r\n",
        "*   Computer vision:: Satellite and drone imagery interpretation (e.g., for disaster resilience); face recognition; image captioning; reading traffic signs; locating pedestrians and vehicles in autonomous vehicles\r\n",
        "*    Medicine:: Finding anomalies in radiology images, including CT, MRI, and X-ray images; counting features in pathology slides; measuring features in ultrasounds; diagnosing diabetic retinopathy\r\n",
        "*    Biology:: Folding proteins; classifying proteins; many genomics tasks, such as tumor-normal sequencing and classifying clinically actionable genetic mutations; cell classification; analyzing protein/protein interactions\r\n",
        "*    Image generation:: Colorizing images; increasing image resolution; removing noise from images; converting images to art in the style of famous artists\r\n",
        "*    Recommendation systems:: Web search; product recommendations; home page layout\r\n",
        "*    Playing games:: Chess, Go, most Atari video games, and many real-time strategy games\r\n",
        "*    Robotics:: Handling objects that are challenging to locate (e.g., transparent, shiny, lacking texture) or hard to pick up\r\n",
        "*    Other applications:: Financial and logistical forecasting, text to speech, and much more...\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IfrhaBqzJdc_"
      },
      "source": [
        "## Some cool examples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_hTYcSu6JiqY"
      },
      "source": [
        "* [DALL-E](https://openai.com/blog/dall-e/): Image generation from text prompts. <br>\r\n",
        "\r\n",
        "![Dall-E](https://analyticsindiamag.com/wp-content/uploads/2021/01/DALLE_AIM.jpg)\r\n",
        "\r\n",
        "* [AlphaFold](https://deepmind.com/blog/article/alphafold-a-solution-to-a-50-year-old-grand-challenge-in-biology): Solution of protein folding problem, an ongoing challenge during the past 50 years.\r\n",
        "\r\n",
        "<img src='https://kstatic.googleusercontent.com/files/7f0ce54218f3f56f78f544146d261f4010f04390e00edf680434a8dc1e34bcb10255605db91a9e339335050a52261ae3523725cc1512095e221befb6f1cf2504' width=\"500\" height=\"750\">\r\n",
        "\r\n",
        "* [AlphaGo](https://www.alphagomovie.com/): An AI capable of beating the best Go players, there is even a movie about it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1aC1TZuNXtlV"
      },
      "source": [
        "# What is Machine Learning?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YVnl24Aw2AFi"
      },
      "source": [
        "We'll let machine learning respond that question. For that reason we wrote a custom prompt in this [web app](https://app.inferkit.com/demo) that said \"**what is machine learning? Machine learning is**\" and got this answer:\r\n",
        "\r\n",
        "\\\\\r\n",
        "`What is machine learning? Machine learning is a form of artificial intelligence where computers are given experiences and the computer \"learns\" from the experience and improves its own future predictions. Essentially, it lets the computer draw conclusions from the patterns it has observed in the past. This is useful for all sorts of reasons, such as automatic speech recognition (for example) or weather forecasts. It is an area of computing that is set to dominate the future and has been given a lot of attention in the tech industry.`\r\n",
        "\r\n",
        "\\\\\r\n",
        "That is a very good description. As the statement implies, machine learning is basically a set of methods to program computers in order that they can learn from examples. For that model to be able to reproduce a text as the previous ones it required lots and lots of text data and adjust itself internally based on a score of some sort, we'll later talk about that. In this picture is illustrated the building blocks of a machine learning model.\r\n",
        "\r\n",
        "\\\\\r\n",
        "<img src='https://drive.google.com/uc?id=1JIpwDf4ZFxcLkTwYoY-ij9UBDsbkeoz-'>\r\n",
        "\r\n",
        "Deep learning is the field inside of machine learning that uses Neural Networks as its principal building block for models. Deep learning also introduces a paradigm shift on how machine learning is done: In traditional machine learning, the input data has to be pre-processed extensively and the features to be learned by the algorithm has to be manually defined. In Deep Learning, these features or representations are learned automatically by the model, in exchange, deep learning models tendo to need more data and computational resources in most cases.\r\n",
        "\r\n",
        "\\\\\r\n",
        "<img src='https://drive.google.com/uc?id=1K3zwaC6uTlfu4n_YdnWTtrD1lIzFEPxm' width=\"500\" height=\"300\">\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l4OLDPeECWFN"
      },
      "source": [
        "## Introduction to Neural Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z1N7rLRZCbUN"
      },
      "source": [
        "In 1943 Warren McCulloch, a neurophysiologist, and Walter Pitts, a logician, teamed up to develop a mathematical model of an artificial neuron. In their [paper](https://link.springer.com/article/10.1007/BF02478259) \"A Logical Calculus of the Ideas Immanent in Nervous Activity\" they declared that:\r\n",
        "\r\n",
        "\r\n",
        "`Because of the “all-or-none” character of nervous activity, neural events and the relations among them can be treated by means of propositional logic. It is found that the behavior of every net can be described in these terms.`\r\n",
        "\r\n",
        "\r\n",
        "McCulloch and Pitts realized that a simplified model of a real neuron could be represented using simple addition and thresholding, as shown here:\r\n",
        "\r\n",
        "<img src=\"https://appliedgo.net/media/perceptron/neuron.png\">\r\n",
        "\r\n",
        "Rosenblatt further developed the artificial neuron to give it the ability to learn. Even more importantly, he worked on building the first device that actually used these principles, the Mark I Perceptron. In \"The Design of an Intelligent Automaton\" Rosenblatt wrote about this work: \"We are now about to witness the birth of such a machine–-a machine capable of perceiving, recognizing and identifying its surroundings without any human training or control.\" The perceptron was built, and was able to successfully recognize simple shapes.\r\n",
        "\r\n",
        "An MIT professor named Marvin Minsky (who was a grade behind Rosenblatt at the same high school!), along with Seymour Papert, wrote a book called Perceptrons (MIT Press), about Rosenblatt's invention. They showed that a single layer of these devices was unable to learn some simple but critical mathematical functions (such as XOR). In the same book, they also showed that using multiple layers of the devices would allow these limitations to be addressed. Unfortunately, only the first of these insights was widely recognized. As a result, the global academic community nearly entirely gave up on neural networks for the next two decades.\r\n",
        "\r\n",
        "\\\\\r\n",
        "\r\n",
        "<img src=\"https://pythonmachinelearning.pro/wp-content/uploads/2017/09/Single-Perceptron.png.webp\" width=550 height=350>\r\n",
        "\r\n",
        "\\\\\r\n",
        "So in theory by coupling many layers of perceptrons, one could achieve an universal function approximator, in theory. But at that time these networks were very slow due to hardware and algorithms limitations. It was not until the last decade that the use of Neural Networks has been gaining exponential interest thanks to the availability of data, hardware development and new algorithms that improved that training in parallel of these models.\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E1vKLpmNIkGL"
      },
      "source": [
        "## The Multi-Layer Perceptron"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ld_0xEKUJn5t"
      },
      "source": [
        "So, the first and most basic neural network model that we will study is the Multi-layer perceptron architecture. And as the name implies, is just a bunch of perceptrons stacked together in many layers.\r\n",
        "\r\n",
        "<img src=\"https://elogeel.files.wordpress.com/2010/05/051010_0921_multilayerp1.png?w=700\">\r\n",
        "\r\n",
        "Oh and by the way, it is useful now to introduce some of the Deep learning jargon employed by the community:\r\n",
        "\r\n",
        "\r\n",
        "*   The functional form of the model is called its architecture (but be careful—sometimes people use model as a synonym of architecture, so this can get confusing).\r\n",
        "*    The weights are called parameters.\r\n",
        "*   The predictions are calculated from the independent variable, which is the data not including the labels.\r\n",
        "*   The results of the model are called predictions.\r\n",
        "*   The measure of performance is called the loss.\r\n",
        "*    The loss depends not only on the predictions, but also the correct labels (also known as targets or the dependent variable); e.g., \"dog\" or \"cat.\"\r\n",
        "\r\n",
        "Remember the Machine learning diagrama from before? Now it looks like this using this terminology:\r\n",
        "\r\n",
        " \\\\\r\n",
        "<img src=\"https://drive.google.com/uc?id=17wBB_w4Y929k3AvYzvstS7zknz683zAU\">\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61ZO3kcALfnc"
      },
      "source": [
        "## How these models learn?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TQSGcM5gLmO8"
      },
      "source": [
        "After we feed the model data and it outputs a prediction, we need to pass that value to a score we want to optimize, this score is usually called the \"loss\" of a model, and the training process is basically the optimization of this value.\r\n",
        "\r\n",
        "\\\\\r\n",
        "\r\n",
        "The optimization process for Neural Networks is based on [gradient descent](https://en.wikipedia.org/wiki/Gradient_descent).\r\n",
        "\r\n",
        "\\\\\r\n",
        "\r\n",
        "<img src=\"https://i.ytimg.com/vi/b4Vyma9wPHo/maxresdefault.jpg\" width=500 height=300>\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C3bEo1SwWSyj"
      },
      "source": [
        "________________________\r\n",
        "\r\n",
        "Now, we will see a gradient descent eaxmple using python. Using python and numpy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cSZjOIueIp-Z"
      },
      "source": [
        "import numpy as np\r\n",
        "\r\n",
        "def gradient_descent(\r\n",
        "\r\n",
        "    gradient, start, learn_rate, n_iter=50, tolerance=1e-06\r\n",
        "\r\n",
        "):\r\n",
        "\r\n",
        "    vector = start\r\n",
        "\r\n",
        "    for _ in range(n_iter):\r\n",
        "\r\n",
        "        diff = -learn_rate * gradient(vector)\r\n",
        "\r\n",
        "        if np.all(np.abs(diff) <= tolerance):\r\n",
        "\r\n",
        "            break\r\n",
        "\r\n",
        "        vector += diff\r\n",
        "\r\n",
        "    return vector"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-KmdnF_YXoHc"
      },
      "source": [
        "\r\n",
        "*    **gradient** is the function or any Python callable object that takes a vector and returns the gradient of the function you’re trying to minimize.\r\n",
        "*    **start** is the point where the algorithm starts its search, given as a sequence (tuple, list, NumPy array, and so on) or scalar (in the case of a one-dimensional problem).\r\n",
        "*    **learn_rate** is the learning rate that controls the magnitude of the vector update.\r\n",
        "*   **n_iter** is the number of iterations.\r\n",
        "\r\n",
        "\\\\\r\n",
        "\r\n",
        "We will use this algorithm to find the minimum of a simple convex function:\r\n",
        "\r\n",
        "$y = x^2$\r\n",
        "\r\n",
        "which we know it's gradient is:\r\n",
        "\r\n",
        "$y' = 2x$\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NNpwvBL7XOmE"
      },
      "source": [
        "y_prime = lambda x: 2*x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JMM1StaReFi8"
      },
      "source": [
        "Here, we use gradient descent to find the minimum of a function. And plot the different steps it takes until it finds the minimum. Try to adjust the learning rate to see what happens."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vTBUrCk_atBh"
      },
      "source": [
        "%matplotlib inline\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "\r\n",
        "#Parameters for Gradient descent\r\n",
        "start = 10\r\n",
        "learn_rate = 0.2\r\n",
        "tol = 1e-6\r\n",
        "n_steps = 50\r\n",
        "\r\n",
        "#Plot of the function\r\n",
        "x = np.linspace(-10,10)\r\n",
        "y = lambda x: x**2\r\n",
        "plt.plot(x,y(x))\r\n",
        "\r\n",
        "#The optimization loop, we do it in such a way to obtain the result in each step.\r\n",
        "results =[start]\r\n",
        "for step in range(n_steps):\r\n",
        "  \r\n",
        "  min = gradient_descent(y_prime,start,learn_rate,1)\r\n",
        "  results.append(min)\r\n",
        "  if np.abs(start-min) < tol:\r\n",
        "    break\r\n",
        "  start = min\r\n",
        "\r\n",
        "results = np.array(results)\r\n",
        "plt.plot(results, y(results), '-*g')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AmJdKNpNfG_T"
      },
      "source": [
        "Try to change the function to a non convex one and it's gradient to see how it behaves!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V7_J98A8feRz"
      },
      "source": [
        "_________________________________________\r\n",
        "Gradient Descent is a simple and fast way to find a minimum, but when the number of parameters of a function increases and when there is non-convexity then the algorithm requires some modifications to work properly."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3NOTLZUz6A6E"
      },
      "source": [
        "## Stochastic Gradient Descent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5_AbPLO46EVb"
      },
      "source": [
        "Stochastic gradient descent algorithms are a modification of gradient descent. In stochastic gradient descent, you calculate the gradient using just a random small part of the observations instead of all of them. In some cases, this approach can reduce computation time.\r\n",
        "\r\n",
        "Online stochastic gradient descent is a variant of stochastic gradient descent in which you estimate the gradient of the cost function for each observation and update the decision variables accordingly. This can help you find the global minimum, especially if the objective function is convex.\r\n",
        "\r\n",
        "Batch stochastic gradient descent is somewhere between ordinary gradient descent and the online method. The gradients are calculated and the decision variables are updated iteratively with subsets of all observations, called minibatches. This variant is very popular for training neural networks.\r\n",
        "\r\n",
        "You can imagine the online algorithm as a special kind of batch algorithm in which each minibatch has only one observation. Classical gradient descent is another special case in which there’s only one batch containing all observations.\r\n",
        "\r\n",
        "From now on we will refer to the minibatch stocasthic gradient descent as stochastic gradient descent (SGD)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iv5l9QLn7idA"
      },
      "source": [
        "### Linear Regression with SGD"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qxpGtS0oZkiV"
      },
      "source": [
        "#### Simple Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cW2jOofWZo3n"
      },
      "source": [
        "We will start by a [simple Linear regression problem](https://en.wikipedia.org/wiki/Simple_linear_regression). In this problem we want to fit a distribution of points in 2D space with a curve. This curve can be linear, a polynomial of any order or even an exponential; the selection of curve depends on the nature of the data we have at hand, and to some extent this represents a limititation of doing regression in this fashion, we need certain prior information of the data befor applying the model, as opposed to the use of neural networks where we can learn that this distribution (but let's leave this to a further topic).\r\n",
        "\r\n",
        "\\\\\r\n",
        "\r\n",
        "First, we will do Linear regression in a classical way using the Least squares method. In linear regresion we want to the determine the slope and intercept (weight and bias) of a line equation:\r\n",
        "\r\n",
        "\\begin{equation}\r\n",
        "  \\hat{y} = b + w x\r\n",
        "\\end{equation}\r\n",
        "\r\n",
        "\\\\\r\n",
        "\r\n",
        "In the least squares method, we want to minimize the squared error between predictions and observations:\r\n",
        "\r\n",
        "\\begin{equation}\r\n",
        "  error = \\frac{1}{n} \\sum^n{(y-\\hat{y})^2}\r\n",
        "\\end{equation}\r\n",
        "\r\n",
        "\\\\\r\n",
        "\r\n",
        "We then can find the weight and bias just by taking the derivative of the error w.r.t. the parameters of our curve and equal to zero. We now are going to derive the formula for finding these values but first we will reformulate or curve equation in a matrix way:\r\n",
        "\r\n",
        "\\begin{equation}\r\n",
        "  \\hat{y} = \\begin{bmatrix}1 & x\\end{bmatrix} . \\begin{bmatrix}b\\\\w\\end{bmatrix}\r\n",
        "\\end{equation}\r\n",
        " \\\\\r\n",
        "Now this reduces to a linear system, that can be rewritten as  $\\hat{y}=X\\theta$, where $\\theta$ is the parameters vector of our model and X is the set of independent variables. We can now rewrite the error as:\r\n",
        "\r\n",
        "\r\n",
        "\\begin{equation}\r\n",
        "  error = (Y-X\\theta)^2\\\\\r\n",
        "  error = (Y-X\\theta)^T(Y-X\\theta)\\\\\r\n",
        "  error = Y^TY - \\theta^TX^TY - Y^TX\\theta + \\theta^TX^TX\\theta\\\\\r\n",
        "  error = Y^TY - 2\\theta^TX^TY + \\theta^TX^TX\\theta\r\n",
        "\\end{equation}\r\n",
        "\r\n",
        "\\\\\r\n",
        "\r\n",
        "This reduction is possible because $\\theta^TX^TY = Y^TX\\theta$, which are scalars and their transpose is the same scalar:\r\n",
        "\r\n",
        "\\begin{equation}\r\n",
        "  (\\theta^TX^TY)^T = (X^TY)^T\\theta = Y^TX\\theta\r\n",
        "\\end{equation}\r\n",
        "\r\n",
        "\\\\\r\n",
        "\r\n",
        "The the derivative of the error w.r.t. the parameters is:\r\n",
        "\r\n",
        "\\begin{equation}\r\n",
        "  \\partial{error}/\\partial{\\theta} = -2X^TY + X^TX\\theta + \\theta^TX^TX = -2X^TY + 2X^TX\\theta\r\n",
        "\\end{equation}\r\n",
        "\r\n",
        "\\\\\r\n",
        "\r\n",
        "Equalizing the derivative to zero when then have:\r\n",
        "\r\n",
        "\\begin{equation}\r\n",
        "  X^TY = X^TX\\theta\\\\\r\n",
        "  \\theta = (X^TX)^{-1}X^TY\r\n",
        "\\end{equation}\r\n",
        "\r\n",
        "\\\\\r\n",
        "\r\n",
        "Now we have a general formula for fitting the parameters, for any linear function!\r\n",
        "\r\n",
        "Now let's solve a simple problems, first we need data to fit. For this reason we will generate some points from a cuadractic function, but we will add some random noise:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QNb6Eg9qKyVa"
      },
      "source": [
        "n_samples = 20\r\n",
        "X = np.sort(np.random.uniform(0,10,(n_samples)))\r\n",
        "y = lambda x: x**2 + np.random.uniform(-10,10,(len(x)))\r\n",
        "Y = y(X)\r\n",
        "plt.scatter(X,Y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e0zQLywRqDgg"
      },
      "source": [
        "Here we make a function to calculate the vector of parameters based on the previous formula:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tJawFKr-NnYM"
      },
      "source": [
        "def lssr(Y, X):\r\n",
        "  X = np.vstack((np.ones(np.size(X,axis=-1)),X)).transpose()\r\n",
        "  B = np.matmul(X.transpose(),X)\r\n",
        "  B = np.matmul(np.linalg.inv(B),X.transpose())\r\n",
        "  return np.matmul(B,Y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GvNnotqyqOH-"
      },
      "source": [
        "Determine the parameters of our curve:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i4ThS2zOR1cH"
      },
      "source": [
        "Theta = lssr(Y,X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OW9C4gsTqZv4"
      },
      "source": [
        "Now visualize the fitted line and the observable points."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RqMPf2EwULd4"
      },
      "source": [
        "plt.scatter(X,Y)\r\n",
        "fit = Theta[0] + Theta[1]*X\r\n",
        "plt.plot(X,fit,'r')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "frbv5NKhqi1H"
      },
      "source": [
        "Now we are goind to try to fit the points but with a cuadratic function: $y = b + w_1x + w_2x^2$. Use the function we created before to obtain the parameters of this curve."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gq0PqeH0Ul4N"
      },
      "source": [
        "x2 = X**2\r\n",
        "X = np.vstack((X,x2))\r\n",
        "Theta_cuadratic = lssr(Y,X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2lsItQgrrASN"
      },
      "source": [
        "Now plot the results, is this model better suited to the data?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bQd8bV9ZVnMl"
      },
      "source": [
        "plt.scatter(X[0],Y)\r\n",
        "fit = Theta_cuadratic[0] + Theta_cuadratic[1]*X[0] + Theta_cuadratic[2]*X[1]\r\n",
        "plt.plot(X[0],fit,'r')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MM4x2utQrO9R"
      },
      "source": [
        "Now we are going to use gradient descent, to solve this same problem. First we have to create a function ta updates the coefficients based on a gradient descent step. Remember that we are trying to minimize de error function, in gradient descent we don't need an analytic formula of the gradient so we can simply take as gradient: \r\n",
        "\\begin{equation}\r\n",
        "\\partial{error}/\\partial{\\theta} = \\frac{1}{n}\\sum^n 2(\\hat{Y}-Y)X\r\n",
        "\\end{equation}\r\n",
        "\\\\\r\n",
        "being $n$ the number of observations. Now we write the function:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4FDF7dFfX05Z"
      },
      "source": [
        "def sgd_update(X, Y, Theta, l_rate):\r\n",
        "      X = np.vstack((np.ones(np.size(X,axis=-1)),X)).transpose()\r\n",
        "      Yhat = np.matmul(X,Theta)\r\n",
        "      grad_error = np.mean(np.einsum('i,ij->ij',2*(Yhat-Y),X), axis = 0)\r\n",
        "      Theta  = Theta - l_rate * grad_error\r\n",
        "      return Theta"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lZsA8ix5tXo4"
      },
      "source": [
        "In this part we find the optimun using SGD:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QtGjCwq2amnU"
      },
      "source": [
        "Theta_sgd = np.zeros(2)\r\n",
        "n_epochs = 2000\r\n",
        "lr = 0.01\r\n",
        "for i in range(n_epochs):\r\n",
        "  y = sgd_update(X[0],Y,Theta_sgd,lr)\r\n",
        "  diff = np.abs(np.sum(y-Theta_sgd))\r\n",
        "  Theta_sgd = y\r\n",
        "  if diff < 1e-6:\r\n",
        "    break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pVN6t7gltcIg"
      },
      "source": [
        "Now we plot the results:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OZZ40moLcOUl"
      },
      "source": [
        "plt.scatter(X[0],Y)\r\n",
        "fit = Theta_sgd[0] + Theta_sgd[1]*X[0]\r\n",
        "plt.plot(X[0],fit,'r')\r\n",
        "error_linear = np.mean((Y - fit)**2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y4pAme8b7i7o"
      },
      "source": [
        "print(error_linear)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D2ealy5BtfWp"
      },
      "source": [
        "We compare the results obtained with the analytical way and using SGD:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ENxzPbiRdr8f"
      },
      "source": [
        "print('result with SGD:')\r\n",
        "print(Theta_sgd)\r\n",
        "print('result in analytical way:')\r\n",
        "print(Theta)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w8VHLPCSt8v_"
      },
      "source": [
        "Now we are going to fit a line with some added gaussian noise:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zx3gL5OohLKV"
      },
      "source": [
        "n_samples = 50\r\n",
        "X = np.sort(np.random.uniform(-10,10,(n_samples)))\r\n",
        "y = lambda x: x + np.random.normal(0,1,n_samples)\r\n",
        "Y = y(X)\r\n",
        "plt.scatter(X,Y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AGmVj4En8SlJ"
      },
      "source": [
        "coef_sgd = np.zeros(2)\r\n",
        "n_epochs = 2000\r\n",
        "lr = 0.01\r\n",
        "for i in range(n_epochs):\r\n",
        "  y = sgd_update(X,Y,coef_sgd,lr)\r\n",
        "  diff = np.abs(np.sum(y-coef_sgd))\r\n",
        "  coef_sgd = y\r\n",
        "  if diff < 1e-6:\r\n",
        "    break\r\n",
        "\r\n",
        "plt.scatter(X,Y)\r\n",
        "fit = coef_sgd[0] + coef_sgd[1]*X\r\n",
        "plt.plot(X,fit,'r')\r\n",
        "error_linear = np.mean((Y - fit)**2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eKZmOrCo8dgu"
      },
      "source": [
        "print(error_linear)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gn6z78e6uDRq"
      },
      "source": [
        "But this time we are going to use a linear polynomial or third order and use SGD:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mwz6kQEXhhbQ"
      },
      "source": [
        "coef_sgd = np.zeros(4)\r\n",
        "#Based on what you have learned so far, fit the points using a third order polynomial: y = b + w1*x + w2*x**2 + w3*x**3 "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1jKtAbq6uP-G"
      },
      "source": [
        "Now we plot the results, what happended?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KrcOPT_ViRLP"
      },
      "source": [
        "#plot the points and the fitted polynomial\r\n",
        "#Also, calculate de Mean Squared error\r\n",
        "error_cubic = "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0jQC-AOI7KCW"
      },
      "source": [
        "print(error_cubic)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4nRihBY0uchG"
      },
      "source": [
        "This phenomenon is called [overfitting](https://en.wikipedia.org/wiki/Overfitting). Here it happened because our model has more parameters than need for the data, it fitted exactly some points of the distribution but in certain areas the error may be to high, resulting in a higher MSE."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CfFg4-sYZaFz"
      },
      "source": [
        "#### Multi_Dimensional Linear Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aFR923Ph8Hwk"
      },
      "source": [
        "In this part we will try to solve a [Linear regression](https://en.wikipedia.org/wiki/Linear_regression) problem using SGD programmed from scratch.\r\n",
        "After we develop our linear regression algorithm with stochastic gradient descent, we will use it to model the wine quality dataset.\r\n",
        "\r\n",
        "This dataset is comprised of the details of 4,898 white wines including measurements like acidity and pH. The goal is to use these objective measures to predict the wine quality on a scale between 0 and 10.\r\n",
        "\r\n",
        "Each columm of the dataset represent the following attributes:\r\n",
        "\r\n",
        "Attribute information:\r\n",
        "\r\n",
        "   Input variables (based on physicochemical tests):\r\n",
        "   1. fixed acidity\r\n",
        "   2. volatile acidity\r\n",
        "   3. citric acid\r\n",
        "   4. residual sugar\r\n",
        "   5. chlorides\r\n",
        "   6. free sulfur dioxide\r\n",
        "   7. total sulfur dioxide\r\n",
        "   8. density\r\n",
        "   9. pH\r\n",
        "   10. sulphates\r\n",
        "   11. alcohol\r\n",
        "   Output variable (based on sensory data): \r\n",
        "   12. quality (score between 0 and 10)\r\n",
        "\r\n",
        "Click [here](https://drive.google.com/file/d/1KKvWK7pmDtJXN5zEA7w0Xje-qbP2FrJI/view?usp=sharing) to download the dataset and the [description](https://drive.google.com/file/d/1dhXvJgR5TRWCk69VTpOznrlyIyTfG3JE/view?usp=sharing).\r\n",
        "\r\n",
        "the function we are trying to fit with this method it is:\r\n",
        "\r\n",
        "\\begin{equation}\r\n",
        "  \\hat{Y} = b_0 + W^T X = X\\theta\r\n",
        "\\end{equation}\r\n",
        "\r\n",
        "Where b_0 is a bias, W is the weight vector and X is the vector of independent variables.\r\n",
        "\r\n",
        "\\\\\r\n",
        "\r\n",
        "In this case we will use mini-batches in stochastic gradient descent for calculating the gradients and updating, the motivation for this is that by doing it this way we are dealing with matrixes of reduced size. The forumula for mini-batch of SGD changes to:\r\n",
        "\r\n",
        "\\begin{equation}\r\n",
        "  \\theta_k = \\theta_{k-1} - lr * \\frac{1}{b}\\sum_1^b 2(\\hat{Y}-Y)X \\\\\r\n",
        "  b < size \\ of \\ the \\ dataset\r\n",
        "\\end{equation}\r\n",
        "\r\n",
        "With this two terms arise:\r\n",
        "\r\n",
        "* Iteration: Is a SGD update step with a batch.\r\n",
        "* Epoch: an epoch completes when we iterate throug the whole training dataset. For better performance, the batches elements are picked randomly from the dataset, this helps with convergence.\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N-O4xSxBf5VU"
      },
      "source": [
        "# Linear Regression With Stochastic Gradient Descent for Wine Quality\r\n",
        "from random import seed\r\n",
        "from random import randrange\r\n",
        "from csv import reader\r\n",
        "from math import sqrt\r\n",
        "from math import ceil\r\n",
        "from numpy import genfromtxt\r\n",
        "from random import shuffle\r\n",
        "\r\n",
        "# Find the min and max values for each column\r\n",
        "def dataset_minmax(dataset):\r\n",
        "\tminmax = list()\r\n",
        "\tfor i in range(len(dataset[0])):\r\n",
        "\t\tcol_values = dataset[:,i]\r\n",
        "\t\tvalue_min = col_values.min()\r\n",
        "\t\tvalue_max = col_values.max()\r\n",
        "\t\tminmax.append([value_min, value_max])\r\n",
        "\treturn minmax\r\n",
        " \r\n",
        "# Rescale dataset columns to the range 0-1\r\n",
        "def normalize_dataset(dataset, minmax):\r\n",
        "  for i in range(len(dataset[0])):\r\n",
        "\t\t\tdataset[:,i] = (dataset[:,i] - minmax[i][0]) / (minmax[i][1] - minmax[i][0])\r\n",
        "  return dataset\r\n",
        "\r\n",
        "# Make a prediction with coefficients\r\n",
        "def predict(batch, coefficients):\r\n",
        "    yhat = np.matmul(batch[:,:-1],coefficients[1:]) + coefficients[0]\r\n",
        "    return yhat\r\n",
        "\r\n",
        "# Estimate linear regression coefficients using stochastic gradient descent\r\n",
        "def sgd_update(batch, Theta, l_rate):\r\n",
        "      X = batch[:,:-1].transpose()\r\n",
        "      Y = batch[:,-1]\r\n",
        "      X = np.vstack((np.ones(np.size(X,axis=-1)),X)).transpose()\r\n",
        "      Yhat = np.matmul(X,Theta)\r\n",
        "      grad_error = np.mean(np.einsum('i,ij->ij',2*(Yhat-Y),X), axis = 0)\r\n",
        "      Theta  = Theta - l_rate * grad_error\r\n",
        "      return Theta"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LhReyNBDQVGj"
      },
      "source": [
        "#Evaluation Metric\r\n",
        "def mse_metric(actual, predicted):\r\n",
        "  return np.mean((actual-predicted)**2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6FZk1re-Qtvc"
      },
      "source": [
        "# Linear Regression on wine quality dataset\r\n",
        "seed(1)\r\n",
        "# load and prepare data\r\n",
        "filename = 'winequality-white.csv'\r\n",
        "#dataset = load_csv(filename)\r\n",
        "dataset = genfromtxt(filename, delimiter=';')\r\n",
        "# normalize\r\n",
        "minmax = dataset_minmax(dataset)\r\n",
        "dataset = normalize_dataset(dataset, minmax)\r\n",
        "train_data = dataset[:4000]\r\n",
        "test_data = dataset[4000:]\r\n",
        "#Training paramateres\r\n",
        "n_epochs = 500\r\n",
        "batch_size = 500\r\n",
        "l_rate = 0.1\r\n",
        "#Initialize Coefficients with zeros\r\n",
        "coef = np.zeros((len(dataset[0])))\r\n",
        "#indexes of rows\r\n",
        "idx = list(range(len(train_data[:,0])))\r\n",
        "n_iterations = ceil(len(train_data[:,0])/batch_size)\r\n",
        "#training loop\r\n",
        "loss = []\r\n",
        "for i in range(n_epochs):\r\n",
        "  shuffle(idx)\r\n",
        "  for j in range(n_iterations):\r\n",
        "    if j==n_iterations-1:\r\n",
        "      batch = train_data[idx[j*batch_size:]]\r\n",
        "    else:\r\n",
        "      batch = train_data[idx[j*batch_size:(j+1)*batch_size]]\r\n",
        "    coef = sgd_update(batch, coef, l_rate)\r\n",
        "    prediction = predict(batch, coef)\r\n",
        "    mse = mse_metric(batch[:,-1],prediction)\r\n",
        "    loss.append(mse)\r\n",
        "    if j%50 == 0 :\r\n",
        "      print(\"epoch: %d Iteration: %d/%d MSE: %.4f \\n\" % ((i+1), (j+1), n_iterations, mse))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JebM9gwcnvjl"
      },
      "source": [
        "plt.plot(loss)\r\n",
        "plt.title(\"MSE\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6JgD2NpEOO86"
      },
      "source": [
        "Now we test the coefficients with the test data we didn't used for training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "beKnhDNeON1q"
      },
      "source": [
        "test_predictions = predict(test_data,coef)\r\n",
        "print(mse_metric(test_data[:,-1],test_predictions))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NpKVYeb8-cC9"
      },
      "source": [
        "idxs = np.random.randint(0,np.size(test_data,0),5)\r\n",
        "print('Wine Quality:')\r\n",
        "print(test_data[idxs,-1]*(minmax[-1][1]-minmax[-1][0]) + minmax[-1][0])\r\n",
        "print('Predicted Wine Quality:')\r\n",
        "print(test_predictions[idxs]*(minmax[-1][1]-minmax[-1][0]) + minmax[-1][0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wjgBgzJKZiHn"
      },
      "source": [
        "We obtained a fit for the data with a Mean squared error of 0.014, try changing the batch size and learning rate to see what effect it gives to the optimization process. Maybe the advantage of SGD cannot be seen in this problem but as dimensionality increases it becomes a really useful tool for training Deep learning models."
      ]
    }
  ]
}
