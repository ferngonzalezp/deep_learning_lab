{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DL_Lab_Session3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOsTNX72CuZZOx6YbfXxVCs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ferngonzalezp/deep_learning_lab/blob/main/session3/DL_Lab_Session3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yLyJn_4LBQ5e"
      },
      "source": [
        "# Session 3: Introduction to sequence modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zmhcGUNKBcKw"
      },
      "source": [
        "In this session we are going to deal with sequences, see how we can model different types of problems as sequences and how to deal with them from a machine learning perspective. \n",
        "\n",
        "## Sequencces in real life\n",
        "\n",
        "<img src=\"https://fr.mathworks.com/help/examples/matlab/win64/GetDataFromAudioRecorderObjectExample_01.png\" height=300>\n",
        "\n",
        "<img src=\"https://www.quickanddirtytips.com/sites/default/files/images/490/sentence-length.jpg\" height=300>\n",
        "\n",
        "<img src=\"https://www.sigmaaldrich.com/content/dam/sigma-aldrich/articles/biology/marketing-assets/sanger-sequencing_dna-structure.png\" height=300>\n",
        "\n",
        "<img src=\"https://d2gk6qz8djobw9.cloudfront.net/slider/15949868021163832.jpg\" height=300>\n",
        "\n",
        "## Application of sequences\n",
        "\n",
        "Depending on the type of sequence problem we are treating, there are many ways in how we can model it in terms of inputs and outputs. as shown in the image below:\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1iyG0kjLo2Nbj6Y7zHTTLX2CMxwqu1Vd0\" height=400>\n",
        "\n",
        "We can use neural networks also for this types of problems, but until now the tasks that we were doing doesn't have time dependence between them. Another point is that using standard neural networks for time series is that they may be too big for covering many time steps or we may fail in capturing time dependencies that are too far away from each other. There is one neural network model that broke through those barrries in sequence modeling and started to advance the state of the art in fields like Natural Language processing and that is Recurrent Neural Networks.\n",
        "\n",
        "<img src=\"https://www.drive.google.com/uc?export=view&id=10adlM-TdQNRfl4Tjx_6Wxtrcaqt1_Tox\" height=400>\n",
        "\n",
        "## Recurrent Neural Networks\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9OST-c8AFqq3"
      },
      "source": [
        "Recurrent nerual networks are a family of neural network architectures that are used for sequential data and use the notion of recurrence. They can have many forms in terms of applications but the basic notion is that the parameters of the model are shared through the entire time history and it takes as input the state of the previous time-step.\n",
        "\n",
        "<img src=\"https://www.drive.google.com/uc?export=view&id=1ZU-vB8owBqqhRGo2PBnv5AWn4v3Cram8\" height=200>\n",
        "<img src=\"https://www.drive.google.com/uc?export=view&id=1ELNhN9RTrIHAM22rzBJA8MlnV09CGd3D\" height=200>\n",
        "\n",
        "LSTMs\n",
        "_______________\n",
        "Common Problem with RNNs: Vanishing and exploding gradients tend to appear when a gradient of an output is small or large, so that error is going to propagate through the whole sequence making the learning diverge or stop updating the weights. This vanishing/exploding gradient problem is usally present when dealing with problems with long range dependencies.\n",
        "\n",
        "One way to deal with these problems and address long-range dependencies is by adding gates to the RNN cell. One of the most effectives architectures for this are Long-Short Term Memory networks or LSTMs. \n",
        "\n",
        "<img src=\"https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-chain.png\" width=1200>\n",
        "\n",
        "\\begin{equation}\n",
        "\\begin{array}{l}\n",
        "i_{t}=\\sigma\\left(W_{i i} x_{t}+b_{i i}+W_{h i} h_{t-1}+b_{h i}\\right) \\\\\n",
        "f_{t}=\\sigma\\left(W_{i f} x_{t}+b_{i f}+W_{h f} h_{t-1}+b_{h f}\\right) \\\\\n",
        "g_{t}=\\tanh \\left(W_{i g} x_{t}+b_{i g}+W_{h g} h_{t-1}+b_{h g}\\right) \\\\\n",
        "o_{t}=\\sigma\\left(W_{i o} x_{t}+b_{i o}+W_{h o} h_{t-1}+b_{h o}\\right) \\\\\n",
        "c_{t}=f_{t} \\odot c_{t-1}+i_{t} \\odot g_{t} \\\\\n",
        "h_{t}=o_{t} \\odot \\tanh \\left(c_{t}\\right)\n",
        "\\end{array}\n",
        "\\end{equation}\n",
        "\n",
        "The LSTM has 4 gates that control the flow of information:\n",
        "\n",
        "*   The forget gate $f_t$ that takes the previous hidden state and the current input to determine how much informaion of the hidden state to forget.\n",
        "*   The input gate $i_t$ that controls how much of the new input to retain.\n",
        "*   The external input gate $g_t$ that controls which parts of the hidden step to update.\n",
        "*   The output gate $o_t$ that controls which information to pass to the next time step.\n",
        "\n",
        "The LSTM introduces a cell state, that serves as a memory for the cell of the network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kr5SJZbl4IU8"
      },
      "source": [
        "#Generaring Names with a Character RNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "brh3LhvFEtRz"
      },
      "source": [
        "Tutorial from pytorch [docs](https://pytorch.org/tutorials/intermediate/char_rnn_generation_tutorial.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sILVXFQtAwvU"
      },
      "source": [
        "<img src=\"https://i.imgur.com/jzVrf7f.png\">"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rst0t-st4Upw"
      },
      "source": [
        "!gdown --id 1ccN7lWQTyrH27NsB__7zNqqSVVFzOCe4\n",
        "!unzip ./data.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ywtj4bV_5LTc"
      },
      "source": [
        "from __future__ import unicode_literals, print_function, division\n",
        "from io import open\n",
        "import glob\n",
        "import os\n",
        "import unicodedata\n",
        "import string\n",
        "\n",
        "all_letters = string.ascii_letters + \" .,;'-\"\n",
        "n_letters = len(all_letters) + 1 # Plus EOS marker\n",
        "\n",
        "def findFiles(path): return glob.glob(path)\n",
        "\n",
        "# Turn a Unicode string to plain ASCII, thanks to https://stackoverflow.com/a/518232/2809427\n",
        "def unicodeToAscii(s):\n",
        "    return ''.join(\n",
        "        c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn'\n",
        "        and c in all_letters\n",
        "    )\n",
        "\n",
        "# Read a file and split into lines\n",
        "def readLines(filename):\n",
        "    lines = open(filename, encoding='utf-8').read().strip().split('\\n')\n",
        "    return [unicodeToAscii(line) for line in lines]\n",
        "\n",
        "# Build the category_lines dictionary, a list of lines per category\n",
        "category_lines = {}\n",
        "all_categories = []\n",
        "for filename in findFiles('data/names/*.txt'):\n",
        "    category = os.path.splitext(os.path.basename(filename))[0]\n",
        "    all_categories.append(category)\n",
        "    lines = readLines(filename)\n",
        "    category_lines[category] = lines\n",
        "\n",
        "n_categories = len(all_categories)\n",
        "\n",
        "if n_categories == 0:\n",
        "    raise RuntimeError('Data not found. Make sure that you downloaded data '\n",
        "        'from https://download.pytorch.org/tutorial/data.zip and extract it to '\n",
        "        'the current directory.')\n",
        "\n",
        "print('# categories:', n_categories, all_categories)\n",
        "print(unicodeToAscii(\"O'Néàl\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wgtC7IpX5TCi"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class RNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(RNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.i2h = nn.Linear(n_categories + input_size + hidden_size, hidden_size)\n",
        "        self.i2o = nn.Linear(n_categories + input_size + hidden_size, output_size)\n",
        "        self.o2o = nn.Linear(hidden_size + output_size, output_size)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, category, input, hidden):\n",
        "        input_combined = torch.cat((category, input, hidden), 1)\n",
        "        hidden = self.i2h(input_combined)\n",
        "        output = self.i2o(input_combined)\n",
        "        output_combined = torch.cat((hidden, output), 1)\n",
        "        output = self.o2o(output_combined)\n",
        "        output = self.dropout(output)\n",
        "        output = self.softmax(output)\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, self.hidden_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "in_HW35z5cUC"
      },
      "source": [
        "import random\n",
        "\n",
        "# Random item from a list\n",
        "def randomChoice(l):\n",
        "    return l[random.randint(0, len(l) - 1)]\n",
        "\n",
        "# Get a random category and random line from that category\n",
        "def randomTrainingPair():\n",
        "    category = randomChoice(all_categories)\n",
        "    line = randomChoice(category_lines[category])\n",
        "    return category, line"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "njDv8ZRv5fsF"
      },
      "source": [
        "# One-hot vector for category\n",
        "def categoryTensor(category):\n",
        "    li = all_categories.index(category)\n",
        "    tensor = torch.zeros(1, n_categories)\n",
        "    tensor[0][li] = 1\n",
        "    return tensor\n",
        "\n",
        "# One-hot matrix of first to last letters (not including EOS) for input\n",
        "def inputTensor(line):\n",
        "    tensor = torch.zeros(len(line), 1, n_letters)\n",
        "    for li in range(len(line)):\n",
        "        letter = line[li]\n",
        "        tensor[li][0][all_letters.find(letter)] = 1\n",
        "    return tensor\n",
        "\n",
        "# LongTensor of second letter to end (EOS) for target\n",
        "def targetTensor(line):\n",
        "    letter_indexes = [all_letters.find(line[li]) for li in range(1, len(line))]\n",
        "    letter_indexes.append(n_letters - 1) # EOS\n",
        "    return torch.LongTensor(letter_indexes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vcMMeZ_r5k_y"
      },
      "source": [
        "# Make category, input, and target tensors from a random category, line pair\n",
        "def randomTrainingExample():\n",
        "    category, line = randomTrainingPair()\n",
        "    category_tensor = categoryTensor(category)\n",
        "    input_line_tensor = inputTensor(line)\n",
        "    target_line_tensor = targetTensor(line)\n",
        "    return category_tensor, input_line_tensor, target_line_tensor"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-BQK6SVW54i-"
      },
      "source": [
        "criterion = nn.NLLLoss()\n",
        "\n",
        "learning_rate = 0.0005\n",
        "\n",
        "def train(category_tensor, input_line_tensor, target_line_tensor):\n",
        "    target_line_tensor.unsqueeze_(-1)\n",
        "    hidden = rnn.initHidden()\n",
        "\n",
        "    rnn.zero_grad()\n",
        "\n",
        "    loss = 0\n",
        "\n",
        "    for i in range(input_line_tensor.size(0)):\n",
        "        output, hidden = rnn(category_tensor, input_line_tensor[i], hidden)\n",
        "        l = criterion(output, target_line_tensor[i])\n",
        "        loss += l\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    for p in rnn.parameters():\n",
        "        p.data.add_(p.grad.data, alpha=-learning_rate)\n",
        "\n",
        "    return output, loss.item() / input_line_tensor.size(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g1P2IEtb6LoF"
      },
      "source": [
        "import time\n",
        "import math\n",
        "\n",
        "def timeSince(since):\n",
        "    now = time.time()\n",
        "    s = now - since\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return '%dm %ds' % (m, s)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gdX0tE2h6OeW"
      },
      "source": [
        "rnn = RNN(n_letters, 128, n_letters)\n",
        "\n",
        "n_iters = 100000\n",
        "print_every = 5000\n",
        "plot_every = 500\n",
        "all_losses = []\n",
        "total_loss = 0 # Reset every plot_every iters\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "for iter in range(1, n_iters + 1):\n",
        "    output, loss = train(*randomTrainingExample())\n",
        "    total_loss += loss\n",
        "\n",
        "    if iter % print_every == 0:\n",
        "        print('%s (%d %d%%) %.4f' % (timeSince(start), iter, iter / n_iters * 100, loss))\n",
        "\n",
        "    if iter % plot_every == 0:\n",
        "        all_losses.append(total_loss / plot_every)\n",
        "        total_loss = 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T-aOqWnv7eTN"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(all_losses)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "atlDXZUm7fXU"
      },
      "source": [
        "max_length = 20\n",
        "\n",
        "# Sample from a category and starting letter\n",
        "def sample(category, start_letter='A'):\n",
        "    with torch.no_grad():  # no need to track history in sampling\n",
        "        category_tensor = categoryTensor(category)\n",
        "        input = inputTensor(start_letter)\n",
        "        hidden = rnn.initHidden()\n",
        "\n",
        "        output_name = start_letter\n",
        "\n",
        "        for i in range(max_length):\n",
        "            output, hidden = rnn(category_tensor, input[0], hidden)\n",
        "            topv, topi = output.topk(1)\n",
        "            topi = topi[0][0]\n",
        "            if topi == n_letters - 1:\n",
        "                break\n",
        "            else:\n",
        "                letter = all_letters[topi]\n",
        "                output_name += letter\n",
        "            input = inputTensor(letter)\n",
        "\n",
        "        return output_name\n",
        "\n",
        "# Get multiple samples from one category and multiple starting letters\n",
        "def samples(category, start_letters='ABC'):\n",
        "    for start_letter in start_letters:\n",
        "        print(sample(category, start_letter))\n",
        "\n",
        "samples('Russian', 'RUS')\n",
        "\n",
        "samples('German', 'GER')\n",
        "\n",
        "samples('Spanish', 'SPA')\n",
        "\n",
        "samples('Chinese', 'CHI')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TRH2jBWBoCMl"
      },
      "source": [
        "# Sentiment Analysis using a LSTM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j18OpvZf8HYg"
      },
      "source": [
        "Network Architecture\n",
        "_______________________\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/bentrevett/pytorch-sentiment-analysis/2b666b3cba7d629a2f192c7d9c66fadcc9f0c363/assets/sentiment3.png\" width=400>\n",
        "\n",
        "Bidirectional LSTM\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/bentrevett/pytorch-sentiment-analysis/2b666b3cba7d629a2f192c7d9c66fadcc9f0c363/assets/sentiment4.png\" width=400>\n",
        "\n",
        "Multi-Layer LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-hRmbNG9BLFD"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchtext import datasets\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import DataLoader\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from collections import Counter\n",
        "from torchtext.vocab import Vocab\n",
        "from torch.utils.data.dataset import random_split\n",
        "from torch.nn.utils.rnn import pad_sequence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qd59cyejAPv6"
      },
      "source": [
        "train_iter, test_iter = datasets.IMDB()\n",
        "max_vocab_size = 25000\n",
        "tokenizer = get_tokenizer('spacy')\n",
        "counter = Counter()\n",
        "for (label, line) in train_iter:\n",
        "    counter.update(tokenizer(line))\n",
        "vocab = Vocab(counter, min_freq=1, max_size=max_vocab_size, vectors=\"glove.6B.100d\",\n",
        "              unk_init =torch.Tensor.normal_)\n",
        "\n",
        "text_pipeline = lambda x: [vocab[token] for token in tokenizer(x)]\n",
        "def label_pipeline(x):\n",
        "  if x=='pos':\n",
        "    return 1\n",
        "  else:\n",
        "    return 0\n",
        "\n",
        "def collate_batch(batch):\n",
        "    label_list, text_list, text_lengths = [], [], []\n",
        "    for (_label, _text) in batch:\n",
        "         label_list.append(label_pipeline(_label))\n",
        "         processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n",
        "         text_list.append((processed_text))\n",
        "         text_lengths.append(len(processed_text))\n",
        "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
        "    text_lengths = torch.tensor(text_lengths, dtype=torch.int64)\n",
        "    text_list = pad_sequence(text_list, batch_first=True)\n",
        "    return label_list.to(device), text_list.to(device), text_lengths.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ymADA7yjW7qi"
      },
      "source": [
        "class RNN(nn.Module):\n",
        "  def __init__(self,vocab_size,embed_dim,hidden_dim,num_layers, pad_idx):\n",
        "    super(RNN,self).__init__()\n",
        "    self.num_layers = num_layers\n",
        "    self.hidden_dim = hidden_dim\n",
        "    self.embed = nn.Embedding(vocab_size,embed_dim, padding_idx = pad_idx)\n",
        "\n",
        "    self.rnn = nn.LSTM(input_size=embed_dim, hidden_size = hidden_dim,\n",
        "                        num_layers = num_layers,bidirectional=True, batch_first=True)\n",
        "    \n",
        "    self.fc = nn.Sequential(nn.Dropout(0.3),nn.Linear(2*hidden_dim,1),nn.Sigmoid())\n",
        "\n",
        "  def forward(self,x,h0, text_lengths):\n",
        "    x = self.embed(x)\n",
        "    x = nn.utils.rnn.pack_padded_sequence(x, text_lengths.to('cpu'), batch_first=True, enforce_sorted=False)\n",
        "    x, (hidden, cell) = self.rnn(x,h0)\n",
        "    x, output_lengths = nn.utils.rnn.pad_packed_sequence(x, batch_first=True)\n",
        "    hidden = self.fc(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1))\n",
        "    return hidden\n",
        "  \n",
        "  def init_hidden(self, batch_size):\n",
        "        ''' Initializes hidden state '''\n",
        "        h0 = torch.zeros((self.num_layers*2,batch_size,self.hidden_dim)).to(device)\n",
        "        c0 = torch.zeros((self.num_layers*2,batch_size,self.hidden_dim)).to(device)\n",
        "        hidden = (h0,c0)\n",
        "        return hidden"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A5uAC15-y6yk"
      },
      "source": [
        "def binary_accuracy(preds, y):\n",
        "    \"\"\"\n",
        "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
        "    \"\"\"\n",
        "\n",
        "    #round predictions to the closest integer\n",
        "    rounded_preds = torch.round((preds))\n",
        "    correct = (rounded_preds == y).float() #convert into float for division \n",
        "    acc = correct.sum() / len(correct)\n",
        "    return acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3zu-10MxpWfU"
      },
      "source": [
        "def train(epochs,model, train_dataloader, val_dataloader=None):\n",
        "  model = model\n",
        "  learn_rate = 1e-3\n",
        "  n_epochs = epochs\n",
        "  #DataLoaders\n",
        "  train_loader = train_dataloader\n",
        "  test_loader = test_dataloader\n",
        "\n",
        "  #Loss function and Optimizer\n",
        "  criterion = nn.BCELoss()\n",
        "  optimizer = torch.optim.Adam(model.parameters(),lr=learn_rate)\n",
        "  \n",
        "  #Training Loop\n",
        "  train_loss = []\n",
        "  val_loss = []\n",
        "  metric = []\n",
        "  for epoch in range(n_epochs):  # loop over the dataset multiple times\n",
        "      epoch_loss = 0.0\n",
        "      running_loss = 0.0\n",
        "      for i, (label, text, text_lengths) in enumerate(train_dataloader):\n",
        "          # zero the parameter gradients\n",
        "          h = model.init_hidden(label.shape[0])\n",
        "          optimizer.zero_grad()\n",
        "          # forward + backward + optimize\n",
        "          predicted_label = model(text,h,text_lengths)\n",
        "          loss = criterion(predicted_label.squeeze(), label.float())\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "          epoch_loss += loss.item()\n",
        "          # print statistics\n",
        "          running_loss += loss.item()\n",
        "\n",
        "          if i % 50 == 49:    # print every 50 mini-batches\n",
        "              print('[%d, %5d] loss: %.3f' %\n",
        "                    (epoch + 1, i + 1, running_loss / 50))\n",
        "              running_loss = 0.0\n",
        "      train_loss.append(epoch_loss/(i+1))\n",
        "      #Evaluation of the trained model\n",
        "      epoch_loss = 0.0\n",
        "      acc = 0.0\n",
        "      if val_dataloader:\n",
        "        print(\"validating...\")\n",
        "        with torch.no_grad():\n",
        "            for i, (label, text, text_lengths) in enumerate(val_dataloader):\n",
        "                val_h = model.init_hidden(label.shape[0])\n",
        "                predicted_label = model(text, val_h,text_lengths)\n",
        "                loss = criterion(predicted_label.squeeze(), label.float())\n",
        "                acc += binary_accuracy(predicted_label.squeeze(), label.float()).item()\n",
        "                epoch_loss += loss.item()\n",
        "        acc = acc/(i+1)\n",
        "        print('Accuracy of the network on the test reviews: %d %%' % (acc*100))\n",
        "        metric.append(acc)\n",
        "        val_loss.append(epoch_loss/(i+1))\n",
        "  return train_loss, val_loss, metric"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vn6pqGd8tKgJ"
      },
      "source": [
        "vocab_size = len(vocab)\n",
        "embed_dim = 100\n",
        "hidden_dim = 256\n",
        "num_layers = 2\n",
        "batch_size = 64\n",
        "pad_idx = vocab.stoi['pad']\n",
        "#initialize RNN\n",
        "model = RNN(vocab_size,embed_dim,hidden_dim,num_layers,pad_idx).to(device)\n",
        "print(model)\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cPtwT-tZf9m3"
      },
      "source": [
        "pretrained_embeddings = vocab.vectors\n",
        "\n",
        "print(pretrained_embeddings.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DN4RoikHhUqd"
      },
      "source": [
        "model.embed.weight.data.copy_(pretrained_embeddings)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "17Z-ocdXhYVI"
      },
      "source": [
        "UNK_IDX = vocab.stoi['unk']\n",
        "\n",
        "model.embed.weight.data[UNK_IDX] = torch.zeros(embed_dim)\n",
        "model.embed.weight.data[pad_idx] = torch.zeros(embed_dim)\n",
        "\n",
        "print(model.embed.weight.data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tOs4LT6FzB_s"
      },
      "source": [
        "#Dataloaders\n",
        "train_iter, test_iter = datasets.IMDB()\n",
        "train_dataset = list(train_iter)\n",
        "test_dataset = list(test_iter)\n",
        "num_train = int(len(train_dataset) * 0.95)\n",
        "split_train_, split_valid_ = \\\n",
        "    random_split(train_dataset, [num_train, len(train_dataset) - num_train])\n",
        "\n",
        "train_dataloader = DataLoader(split_train_, batch_size=batch_size, collate_fn=collate_batch,\n",
        "                              shuffle=True)\n",
        "valid_dataloader = DataLoader(split_valid_, batch_size=batch_size, collate_fn=collate_batch,\n",
        "                              shuffle=False)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, collate_fn=collate_batch,\n",
        "                             shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3fG6YSX-wMx-"
      },
      "source": [
        "train_loss, val_loss, metric = train(epochs=5, model=model, train_dataloader=train_dataloader, valid_dataloader=valid_dataloader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xklYmC0o93gr"
      },
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "def predict_sentiment(model, sentence):\n",
        "    model.eval()\n",
        "    tokenized = [tok.text for tok in nlp.tokenizer(sentence)]\n",
        "    indexed = [vocab.stoi[t] for t in tokenized]\n",
        "    length = [len(indexed)]\n",
        "    tensor = torch.LongTensor(indexed).to(device)\n",
        "    tensor = tensor.unsqueeze(0)\n",
        "    length_tensor = torch.LongTensor(length)\n",
        "    prediction = model(tensor,model.init_hidden(1), length_tensor)\n",
        "    if torch.round(prediction) == 0:\n",
        "      print('negative review')\n",
        "    else:\n",
        "      print('positive review')\n",
        "    return prediction.item()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FDzZnUiU-IXU"
      },
      "source": [
        "predict_sentiment(model, \"Should be fired and sued for destroying justice league movie.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8jbJq_4T_wkU"
      },
      "source": [
        "predict_sentiment(model, \"Great movie, would recommend.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VAs9hVuXCHO_"
      },
      "source": [
        "#Exercises\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UUM4sBc0_P3N"
      },
      "source": [
        "## 1. Create a Name Generator using a LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V0ZjRTeJ_eag"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kXSbug4N_fIZ"
      },
      "source": [
        "## 2. Create a Multi-Class Text Classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ncKmAB1kOQv9"
      },
      "source": [
        "For this task we will use the Yahoo Answers dataset, it has questions and 10 categories:\n",
        "\n",
        "<img src=\"https://www.drive.google.com/uc?view=export&id=1Af1S0L4207Yfw9QPXqhcUYCJO5RXei8I\">\n",
        "\n",
        "For this problem we will use a similar LSTM model as before but with some differences:\n",
        "\n",
        "* We will use the CrossEntropyLoss.\n",
        "* The output dimension of the linear layers will be 10.\n",
        "* We need to remove the squeeze() in the prediced labels when calculating the loss.\n",
        "* Remove the conversion t float() of the labels.\n",
        "* Remove the activation of the final linear layer, CrossEntropyLoss does logSoftmax and Negative log-likelihood combined."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-1eoAhabAmFQ"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchtext import datasets\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import DataLoader\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from collections import Counter\n",
        "from torchtext.vocab import Vocab\n",
        "from torch.utils.data.dataset import random_split\n",
        "from torch.nn.utils.rnn import pad_sequence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wSaxaPbbJ_ye"
      },
      "source": [
        "train_iter, test_iter = datasets.YahooAnswers()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O7271vurAunW"
      },
      "source": [
        "max_vocab_size = 25000\n",
        "tokenizer = get_tokenizer('spacy')\n",
        "counter = Counter()\n",
        "for (label, line) in train_iter:\n",
        "    counter.update(tokenizer(line))\n",
        "vocab = Vocab(counter, min_freq=1, max_size=max_vocab_size, vectors=\"glove.6B.100d\",\n",
        "              unk_init =torch.Tensor.normal_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "crsyL6QHBHyD"
      },
      "source": [
        "next(test_iter)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J5xLaEwTKKtv"
      },
      "source": [
        "text_pipeline = lambda x: [vocab[token] for token in tokenizer(x)]\n",
        "label_pipeline = lambda x: int(x) - 1\n",
        "\n",
        "def collate_batch(batch):\n",
        "    label_list, text_list, text_lengths = [], [], []\n",
        "    for (_label, _text) in batch:\n",
        "         label_list.append(label_pipeline(_label))\n",
        "         processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n",
        "         text_list.append((processed_text))\n",
        "         text_lengths.append(len(processed_text))\n",
        "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
        "    text_lengths = torch.tensor(text_lengths, dtype=torch.int64)\n",
        "    text_list = pad_sequence(text_list, batch_first=True)\n",
        "    return label_list.to(device), text_list.to(device), text_lengths.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YsIwM8UoLAvG"
      },
      "source": [
        "[vocab[token] for token in ['here', 'is', 'an', 'example']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u5itndScgHAi"
      },
      "source": [
        "# Build your LSTM model\n",
        "\n",
        "\"To DO\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XRESLcYZLpnE"
      },
      "source": [
        "def categorical_accuracy(preds, y):\n",
        "    \"\"\"\n",
        "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
        "    \"\"\"\n",
        "    top_pred = preds.argmax(1, keepdim = True)\n",
        "    correct = top_pred.eq(y.view_as(top_pred)).sum()\n",
        "    acc = correct.float() / y.shape[0]\n",
        "    return acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ECLskDNwL3V0"
      },
      "source": [
        "def train(epochs,model, train_dataloader, val_dataloader=None):\n",
        "  model = model\n",
        "  learn_rate = 1e-3\n",
        "  n_epochs = epochs\n",
        "  #DataLoaders\n",
        "  train_loader = train_dataloader\n",
        "  test_loader = test_dataloader\n",
        "\n",
        "  #Loss function and Optimizer\n",
        "  criterion = #Define loss Function\n",
        "  optimizer = torch.optim.Adam(model.parameters(),lr=learn_rate)\n",
        "  \n",
        "  #Training Loop\n",
        "  train_loss = []\n",
        "  val_loss = []\n",
        "  metric = []\n",
        "  for epoch in range(n_epochs):  # loop over the dataset multiple times\n",
        "      epoch_loss = 0.0\n",
        "      running_loss = 0.0\n",
        "      for i, (label, text, text_lengths) in enumerate(train_dataloader):\n",
        "          # zero the parameter gradients\n",
        "          h = model.init_hidden(label.shape[0])\n",
        "          optimizer.zero_grad()\n",
        "          # forward + backward + optimize\n",
        "          predicted_label = model(text,h,text_lengths)\n",
        "          loss = criterion(predicted_label, label)\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "          epoch_loss += loss.item()\n",
        "          # print statistics\n",
        "          running_loss += loss.item()\n",
        "\n",
        "          if i % 50 == 49:    # print every 50 mini-batches\n",
        "              print('[%d, %5d] loss: %.3f' %\n",
        "                    (epoch + 1, i + 1, running_loss / 50))\n",
        "              running_loss = 0.0\n",
        "      train_loss.append(epoch_loss/(i+1))\n",
        "      #Evaluation of the trained model\n",
        "      epoch_loss = 0.0\n",
        "      acc = 0.0\n",
        "      if val_dataloader:\n",
        "        print(\"validating...\")\n",
        "        with torch.no_grad():\n",
        "            for i, (label, text, text_lengths) in enumerate(val_dataloader):\n",
        "                val_h = model.init_hidden(label.shape[0])\n",
        "                predicted_label = model(text, val_h,text_lengths)\n",
        "                loss = criterion(predicted_label, label)\n",
        "                acc += categorical_accuracy(predicted_label, label)\n",
        "                epoch_loss += loss.item()\n",
        "        acc = acc.item()/(i+1)\n",
        "        print('Accuracy of the network on the test reviews: %d %%' % (acc*100))\n",
        "        metric.append(acc)\n",
        "        val_loss.append(epoch_loss/(i+1))\n",
        "  return train_loss, val_loss, metric"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5zT7eUhOMKY9"
      },
      "source": [
        "vocab_size = len(vocab)\n",
        "embed_dim = 100\n",
        "hidden_dim = 256\n",
        "num_layers = 2\n",
        "batch_size = 64\n",
        "pad_idx = vocab.stoi['pad']\n",
        "#initialize RNN\n",
        "model = RNN(vocab_size,embed_dim,hidden_dim,num_layers,pad_idx).to(device)\n",
        "print(model)\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FgRB0JaUMg-a"
      },
      "source": [
        "pretrained_embeddings = vocab.vectors\n",
        "\n",
        "print(pretrained_embeddings.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ibh_BQI_MnZO"
      },
      "source": [
        "model.embed.weight.data.copy_(pretrained_embeddings)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BanCi7zNMu1x"
      },
      "source": [
        "UNK_IDX = vocab.stoi['unk']\n",
        "\n",
        "model.embed.weight.data[UNK_IDX] = torch.zeros(embed_dim)\n",
        "model.embed.weight.data[pad_idx] = torch.zeros(embed_dim)\n",
        "\n",
        "print(model.embed.weight.data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ot3ObBorMyiV"
      },
      "source": [
        "#Dataloaders\n",
        "train_iter, test_iter = datasets.YahooAnswers()\n",
        "train_dataset = list(train_iter)\n",
        "test_dataset = list(test_iter)\n",
        "num_train = int(len(train_dataset) * 0.95)\n",
        "split_train_, split_valid_ = \\\n",
        "    random_split(train_dataset, [num_train, len(train_dataset) - num_train])\n",
        "\n",
        "train_dataloader = DataLoader(split_train_, batch_size=batch_size, collate_fn=collate_batch,\n",
        "                              shuffle=True)\n",
        "valid_dataloader = DataLoader(split_valid_, batch_size=batch_size, collate_fn=collate_batch,\n",
        "                              shuffle=False)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, collate_fn=collate_batch,\n",
        "                             shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Of3CJzgYNIRu"
      },
      "source": [
        "train_loss, val_loss, metric = train(epochs=1, model=model, train_dataloader=train_dataloader, val_dataloader=valid_dataloader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h01D16PqgXT5"
      },
      "source": [
        "label, text = test_dataset[np.random.randint(len(test_dataset))]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yAqmG1q4hpSo"
      },
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "def predict_sentiment(model, sentence):\n",
        "    model.eval()\n",
        "    tokenized = [tok.text for tok in nlp.tokenizer(sentence)]\n",
        "    indexed = [vocab.stoi[t] for t in tokenized]\n",
        "    length = [len(indexed)]\n",
        "    tensor = torch.LongTensor(indexed).to(device)\n",
        "    tensor = tensor.unsqueeze(0)\n",
        "    length_tensor = torch.LongTensor(length)\n",
        "    prediction = model(tensor,model.init_hidden(1), length_tensor)\n",
        "    return prediction.argmax(1).item() + 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C98AeROQg-6M"
      },
      "source": [
        "print(text)\n",
        "print(predict_sentiment(model,text))\n",
        "print(label)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}